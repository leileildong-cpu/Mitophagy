#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶…é«˜é€ŸGPUåŠ é€Ÿ + å¤šçº¿ç¨‹CPUä¼˜åŒ–çš„è®ºæ–‡å¤ç°ç‰ˆè™šæ‹Ÿç­›é€‰ç³»ç»Ÿ
åŸºäº"AI-Assisted Discovery of Mitophagy-Inducing Compounds"
åŒ…å«ï¼šå¤šè¡¨ç¤ºå­¦ä¹  + GPUåŠ é€Ÿ + CPUå¤šçº¿ç¨‹ + å®Œæ•´è®ºæ–‡é€»è¾‘ + é€Ÿåº¦ä¼˜åŒ–
ç‰ˆæœ¬: v2.0 Ultra Fast
"""
import os
os.environ['RDK_PICKLE_PROTOCOL'] = '2'  # è®¾ç½®pickleåè®®ç‰ˆæœ¬

import warnings
warnings.filterwarnings('ignore', message='.*Pickling.*')  # å¿½ç•¥pickleè­¦å‘Š
warnings.filterwarnings('ignore', category=DeprecationWarning)

import argparse
import yaml
import numpy as np
import pandas as pd
from tqdm import tqdm
from typing import List, Tuple, Dict, Optional, Union, Iterator
from datetime import datetime
import pickle
import time
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from functools import partial, lru_cache
import threading
from collections import defaultdict
import gc
import psutil

warnings.filterwarnings('ignore')

# æ ¸å¿ƒä¾èµ–
from rdkit import Chem
from rdkit.Chem import AllChem, Descriptors, Crippen, Lipinski, rdMolDescriptors
from rdkit import DataStructs
from rdkit.Chem.Pharm2D import Generate, Gobbi_Pharm2D
from rdkit.Chem.FilterCatalog import FilterCatalog, FilterCatalogParams

# GPUåŠ é€Ÿç›¸å…³
try:
    import cupy as cp
    import cupyx.scipy.sparse as cp_sparse
    from cupyx.scipy.spatial.distance import cdist as cp_cdist

    GPU_AVAILABLE = True
    print("[INFO] GPUåŠ é€Ÿå·²å¯ç”¨ (CuPy)")
except ImportError:
    import numpy as cp

    GPU_AVAILABLE = False
    print("[WARNING] GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®¡ç®—")

# PyTorch GPUæ”¯æŒ
try:
    import torch
    import torch.nn.functional as F

    TORCH_AVAILABLE = torch.cuda.is_available() if torch.cuda.is_available() else False
    if TORCH_AVAILABLE:
        print(f"[INFO] PyTorch GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        DEVICE = torch.device('cuda')
        # ä¼˜åŒ–PyTorchè®¾ç½®
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.set_float32_matmul_precision('high')  # å¯ç”¨TensorFloat-32
    else:
        print("[INFO] PyTorchä½¿ç”¨CPU")
        DEVICE = torch.device('cpu')
except ImportError:
    TORCH_AVAILABLE = False
    print("[WARNING] PyTorchä¸å¯ç”¨")

# æœºå™¨å­¦ä¹ 
try:
    from sklearn.cluster import MiniBatchKMeans  # æ›¿æ¢KMeansä¸ºæ›´å¿«çš„MiniBatchKMeans
    from sklearn.preprocessing import StandardScaler
    from sklearn.ensemble import IsolationForest
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.decomposition import PCA
    from sklearn.manifold import TSNE
    from sklearn.neighbors import NearestNeighbors  # ç”¨äºå¿«é€Ÿç›¸ä¼¼æ€§æœç´¢

    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("[WARNING] scikit-learnä¸å¯ç”¨ï¼ŒAIåŠŸèƒ½ç¦ç”¨")

# Mol2vecï¼ˆè®ºæ–‡æ ¸å¿ƒï¼‰
try:
    from mol2vec.features import mol2alt_sentence, sentences2vec
    from gensim.models import word2vec

    MOL2VEC_AVAILABLE = True
except ImportError:
    MOL2VEC_AVAILABLE = False
    print("[WARNING] mol2vecä¸å¯ç”¨ï¼Œä½¿ç”¨MorganæŒ‡çº¹æ›¿ä»£")

# 3Då½¢çŠ¶ç›¸ä¼¼æ€§
try:
    from rdkit.Chem import rdShapeHelpers
    from rdkit.Chem.rdMolAlign import AlignMol

    SHAPE3D_AVAILABLE = True
except ImportError:
    SHAPE3D_AVAILABLE = False
    print("[WARNING] 3Då½¢çŠ¶ç›¸ä¼¼æ€§ä¸å¯ç”¨")

# åˆ†å­æ ‡å‡†åŒ–
try:
    from rdkit.Chem import rdMolStandardize as rdMS
except ImportError:
    try:
        from rdkit.Chem.MolStandardize import rdMolStandardize as rdMS
    except ImportError:
        rdMS = None


class OptimizedCPUThreadPool:
    """ä¼˜åŒ–çš„CPUå¤šçº¿ç¨‹ç®¡ç†å™¨ - æ›´æ™ºèƒ½çš„ä»»åŠ¡è°ƒåº¦"""

    def __init__(self, max_workers: Optional[int] = None):
        # åŠ¨æ€è®¡ç®—æœ€ä¼˜çº¿ç¨‹æ•°
        cpu_count = os.cpu_count() or 1
        available_memory = psutil.virtual_memory().available / (1024 ** 3)  # GB

        # åŸºäºCPUå’Œå†…å­˜åŠ¨æ€è°ƒæ•´çº¿ç¨‹æ•°
        if max_workers is None:
            if available_memory > 16:  # å¤§å†…å­˜ç³»ç»Ÿ
                self.max_workers = min(32, cpu_count * 2)
            elif available_memory > 8:  # ä¸­ç­‰å†…å­˜
                self.max_workers = min(16, cpu_count + 4)
            else:  # å°å†…å­˜ç³»ç»Ÿ
                self.max_workers = min(8, cpu_count)
        else:
            self.max_workers = max_workers

        print(f"[INFO] ä¼˜åŒ–CPUçº¿ç¨‹æ± : {self.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹ (å¯ç”¨å†…å­˜: {available_memory:.1f}GB)")

        self.process_pool = None
        self.thread_pool = None

    def map_parallel(self, func, items, desc="å¤„ç†ä¸­", use_processes=False, batch_size=None,
                     early_stop_threshold=None):
        """å¢å¼ºçš„å¹¶è¡Œæ˜ å°„å‡½æ•° - æ”¯æŒæ‰¹å¤„ç†å’Œæ—©åœ"""
        if len(items) < 50:  # å°æ•°æ®é›†ç›´æ¥ä¸²è¡Œå¤„ç†
            return [func(item) for item in tqdm(items, desc=desc)]

        # åŠ¨æ€æ‰¹å¤„ç†
        if batch_size is None:
            batch_size = max(1, len(items) // (self.max_workers * 4))

        # é€‰æ‹©æ‰§è¡Œå™¨
        executor_class = ProcessPoolExecutor if use_processes else ThreadPoolExecutor

        results = []
        with executor_class(max_workers=self.max_workers) as executor:
            # æ‰¹é‡æäº¤ä»»åŠ¡
            futures = []
            for i in range(0, len(items), batch_size):
                batch = items[i:i + batch_size]
                if len(batch) == 1:
                    future = executor.submit(func, batch[0])
                else:
                    # æ‰¹å¤„ç†å‡½æ•°
                    batch_func = partial(self._batch_process, func)
                    future = executor.submit(batch_func, batch)
                futures.append(future)

            # æ”¶é›†ç»“æœï¼Œæ”¯æŒæ—©åœ
            with tqdm(total=len(items), desc=f"{desc} (æ‰¹å¤„ç†)") as pbar:
                for future in as_completed(futures):
                    try:
                        batch_results = future.result()
                        if isinstance(batch_results, list):
                            results.extend(batch_results)
                            pbar.update(len(batch_results))
                        else:
                            results.append(batch_results)
                            pbar.update(1)

                        # æ—©åœæ£€æŸ¥
                        if early_stop_threshold and len(results) >= early_stop_threshold:
                            break

                    except Exception as e:
                        print(f"[WARNING] æ‰¹å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
                        continue

        return results[:len(items)]  # ç¡®ä¿è¿”å›æ­£ç¡®æ•°é‡çš„ç»“æœ

    def _batch_process(self, func, batch):
        """æ‰¹å¤„ç†å‡½æ•°"""
        return [func(item) for item in batch]

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if self.process_pool:
            self.process_pool.shutdown(wait=False)
        if self.thread_pool:
            self.thread_pool.shutdown(wait=False)


class FastMolecularFeatureCache:
    """åˆ†å­ç‰¹å¾ç¼“å­˜ç³»ç»Ÿ - ä¿®å¤ç‰ˆ"""

    def __init__(self, max_size=10000):
        self.cache = {}
        self.max_size = max_size
        self.access_count = defaultdict(int)

    def get_key(self, mol):
        """ç”Ÿæˆåˆ†å­çš„ç¼“å­˜é”® - ä¿®å¤ç‰ˆ"""
        if mol is None:
            return None
        try:
            # ä½¿ç”¨SMILESä½œä¸ºé”®ï¼Œè€Œä¸æ˜¯åˆ†å­å¯¹è±¡
            smiles = Chem.MolToSmiles(mol, isomericSmiles=True)
            return smiles
        except Exception as e:
            return None

    def get(self, mol, feature_type):
        """è·å–ç¼“å­˜çš„ç‰¹å¾"""
        key = self.get_key(mol)
        if key is None:
            return None

        cache_key = f"{key}_{feature_type}"
        if cache_key in self.cache:
            self.access_count[cache_key] += 1
            return self.cache[cache_key]
        return None

    def set(self, mol, feature_type, features):
        """è®¾ç½®ç¼“å­˜ - ä¿®å¤ç‰ˆ"""
        key = self.get_key(mol)
        if key is None:
            return

        cache_key = f"{key}_{feature_type}"

        # ç¼“å­˜æ»¡æ—¶æ¸…ç†æœ€å°‘ä½¿ç”¨çš„
        if len(self.cache) >= self.max_size:
            sorted_keys = sorted(self.cache.keys(), key=lambda k: self.access_count.get(k, 0))
            for k in sorted_keys[:self.max_size // 10]:
                if k in self.cache:
                    del self.cache[k]
                if k in self.access_count:
                    del self.access_count[k]

        # ç¡®ä¿featuresæ˜¯å¯åºåˆ—åŒ–çš„
        try:
            # å¦‚æœfeaturesæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºå¯å“ˆå¸Œçš„å½¢å¼
            if isinstance(features, dict):
                # ä¸ç›´æ¥ç¼“å­˜å­—å…¸ï¼Œè€Œæ˜¯ç¼“å­˜å…¶å‰¯æœ¬
                self.cache[cache_key] = features.copy()
            elif isinstance(features, np.ndarray):
                self.cache[cache_key] = features.copy()
            else:
                self.cache[cache_key] = features

            self.access_count[cache_key] = 1
        except Exception as e:
            print(f"[WARNING] ç¼“å­˜è®¾ç½®å¤±è´¥: {e}")


class UltraFastVirtualScreening:
    """è¶…é«˜é€ŸGPUåŠ é€Ÿ + å¤šçº¿ç¨‹CPUä¼˜åŒ–çš„è™šæ‹Ÿç­›é€‰ç³»ç»Ÿ"""

    def __init__(self, config: dict):
        self.config = self._validate_and_setup_config(config)
        self.cpu_pool = OptimizedCPUThreadPool()
        self.feature_cache = FastMolecularFeatureCache(max_size=20000)
        self.setup_gpu()
        self.setup_logging()
        self._precomputed_models = {}

        # æ€§èƒ½ç›‘æ§
        self.perf_stats = {
            'preprocessing_time': 0,
            'feature_extraction_time': 0,
            'similarity_computation_time': 0,
            'filtering_time': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }

    def _validate_and_setup_config(self, config: dict) -> dict:
        """éªŒè¯å’Œè®¾ç½®é…ç½®é»˜è®¤å€¼"""

        # ç¡®ä¿filtersé…ç½®å­˜åœ¨
        if "filters" not in config:
            config["filters"] = {}

        # è®¾ç½®è¿‡æ»¤å™¨é»˜è®¤å€¼
        filters_defaults = {
            "pains": True,
            "cns_filter": True,
            "cns_rules": {
                "mw_min": 130,  # åˆ†å­é‡æœ€å°å€¼
                "mw_max": 725,  # åˆ†å­é‡æœ€å¤§å€¼
                "logp_min": -7,  # LogPæœ€å°å€¼
                "logp_max": 5.5,  # LogPæœ€å¤§å€¼
                "hbd_max": 7,  # æ°¢é”®ä¾›ä½“æœ€å¤§æ•°
                "hba_max": 12,  # æ°¢é”®å—ä½“æœ€å¤§æ•°
                "tpsa_max": 200,  # æ‹“æ‰‘ææ€§è¡¨é¢ç§¯æœ€å¤§å€¼
                "rotb_max": 11  # å¯æ—‹è½¬é”®æœ€å¤§æ•°
            }
        }

        for key, default_value in filters_defaults.items():
            if key not in config["filters"]:
                config["filters"][key] = default_value

        # ç¡®ä¿similarityé…ç½®å­˜åœ¨
        if "similarity" not in config:
            config["similarity"] = {}

        similarity_defaults = {
            "w_1d": 0.4,
            "w_2d": 0.4,
            "w_3d": 0.2
        }

        for key, default_value in similarity_defaults.items():
            if key not in config["similarity"]:
                config["similarity"][key] = default_value

        # ç¡®ä¿ai_modelé…ç½®å­˜åœ¨
        if "ai_model" not in config:
            config["ai_model"] = {}

        ai_model_defaults = {
            "use_clustering": False,
            "use_outlier_filter": False,
            "similarity_threshold": 0.75
        }

        for key, default_value in ai_model_defaults.items():
            if key not in config["ai_model"]:
                config["ai_model"][key] = default_value

        # ç¡®ä¿performanceé…ç½®å­˜åœ¨
        if "performance" not in config:
            config["performance"] = {}

        performance_defaults = {
            "chunk_size": 10000,
            "use_streaming": False,
            "max_results": 50000
        }

        for key, default_value in performance_defaults.items():
            if key not in config["performance"]:
                config["performance"][key] = default_value

        # ç¡®ä¿fingerprintsé…ç½®å­˜åœ¨
        if "fingerprints" not in config:
            config["fingerprints"] = {}

        fingerprints_defaults = {
            "morgan_radius": 2,
            "morgan_nbits": 2048
        }

        for key, default_value in fingerprints_defaults.items():
            if key not in config["fingerprints"]:
                config["fingerprints"][key] = default_value

        # ç¡®ä¿è¾“å‡ºé…ç½®å­˜åœ¨
        if "output" not in config:
            config["output"] = {}

        output_defaults = {
            "dir": "results",
            "hits_csv": "virtual_screening_hits.csv",
            "report_txt": "screening_report.txt"
        }

        for key, default_value in output_defaults.items():
            if key not in config["output"]:
                config["output"][key] = default_value

        return config

    def setup_gpu(self):
        """ä¼˜åŒ–çš„GPUç¯å¢ƒè®¾ç½® - ä¿®å¤å†…å­˜ç®¡ç†"""
        global GPU_AVAILABLE, TORCH_AVAILABLE

        if GPU_AVAILABLE:
            try:
                import cupy as cp
                device_count = cp.cuda.runtime.getDeviceCount()
                current_device = cp.cuda.device.get_device_id()

                # è®¾ç½®å†…å­˜æ± ç­–ç•¥ - æ›´ä¿å®ˆçš„è®¾ç½®
                mempool = cp.get_default_memory_pool()
                pinned_mempool = cp.get_default_pinned_memory_pool()

                # è·å–å†…å­˜ä¿¡æ¯
                free_memory, total_memory = cp.cuda.runtime.memGetInfo()
                # ä½¿ç”¨æ›´ä¿å®ˆçš„å†…å­˜é™åˆ¶ï¼ˆ70%è€Œä¸æ˜¯85%ï¼‰
                memory_limit = int(free_memory * 0.7)
                mempool.set_limit(size=memory_limit)

                self.gpu_info = {
                    'device_count': device_count,
                    'current_device': current_device,
                    'total_memory': total_memory / (1024 ** 3),
                    'free_memory': free_memory / (1024 ** 3),
                    'memory_limit': memory_limit / (1024 ** 3)
                }

                print(f"[GPU] CuPyè®¾ç½®: {self.gpu_info['total_memory']:.1f}GBæ€»é‡, "
                      f"{self.gpu_info['memory_limit']:.1f}GBé™åˆ¶")

            except Exception as e:
                print(f"[WARNING] CuPy GPUè®¾ç½®å¤±è´¥: {e}")
                GPU_AVAILABLE = False

        # PyTorch GPUä¼˜åŒ–è®¾ç½®
        if TORCH_AVAILABLE:
            try:
                # è®¾ç½®PyTorchå†…å­˜ç®¡ç†
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                torch.set_num_threads(min(4, self.cpu_pool.max_workers // 4))

                if torch.cuda.is_available():
                    # æ›´ä¿å®ˆçš„å†…å­˜è®¾ç½®
                    torch.cuda.set_per_process_memory_fraction(0.7)  # ä»0.9æ”¹ä¸º0.7
                    torch.cuda.empty_cache()

                    # ç¦ç”¨å†…å­˜å›ºå®šä»¥é¿å…è­¦å‘Š
                    torch.multiprocessing.set_sharing_strategy('file_system')

                    device_name = torch.cuda.get_device_name(0)
                    print(f"[GPU] PyTorchè®¾ç½®: {device_name}")

            except Exception as e:
                print(f"[WARNING] PyTorch GPUè®¾ç½®å¤±è´¥: {e}")
                TORCH_AVAILABLE = False

        # PyTorch GPUä¼˜åŒ–è®¾ç½®
        if TORCH_AVAILABLE:
            try:
                # æ›´æ¿€è¿›çš„GPUè®¾ç½®
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                torch.set_num_threads(min(4, self.cpu_pool.max_workers // 4))

                if torch.cuda.is_available():
                    # ä½¿ç”¨æ›´å¤šæ˜¾å­˜
                    torch.cuda.set_per_process_memory_fraction(0.9)
                    torch.cuda.empty_cache()

                    # å¯ç”¨å†…å­˜é¢„åˆ†é…
                    torch.cuda.set_per_process_memory_fraction(0.9)

                    device_name = torch.cuda.get_device_name(0)
                    print(f"[GPU] PyTorchä¼˜åŒ–è®¾ç½®: {device_name}")

            except Exception as e:
                print(f"[WARNING] PyTorch GPUä¼˜åŒ–å¤±è´¥: {e}")
                TORCH_AVAILABLE = False



    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        original_dir = self.config["output"]["dir"]
        self.output_dir = f"{original_dir}_final_{timestamp}"
        self.config["output"]["dir"] = self.output_dir

        os.makedirs(self.output_dir, exist_ok=True)

        # ä¿å­˜é…ç½®æ–‡ä»¶å‰¯æœ¬
        config_backup = os.path.join(self.output_dir, "config_used.yaml")
        with open(config_backup, 'w') as f:
            yaml.dump(self.config, f)

    def load_and_preprocess_data_streaming(self) -> Tuple[pd.DataFrame, Iterator]:
        """æµå¼æ•°æ®åŠ è½½å’Œé¢„å¤„ç† - å¤„ç†è¶…å¤§æ•°æ®é›†"""
        print("[INFO] å¯åŠ¨æµå¼æ•°æ®åŠ è½½...")
        start_time = time.time()

        # è¯»å–å‚è€ƒæ•°æ®ï¼ˆé€šå¸¸è¾ƒå°ï¼‰
        ref_df = pd.read_csv(self.config["data"]["references_csv"])
        print(f"[INFO] å‚è€ƒåˆ†å­: {len(ref_df)}")

        # éªŒè¯å¿…éœ€åˆ—
        required_cols = {"id", "name", "smiles"}
        for df, name in [(ref_df, "references")]:
            if not required_cols.issubset(set(df.columns)):
                raise ValueError(f"{name} ç¼ºå°‘å¿…éœ€åˆ—: {required_cols - set(df.columns)}")

        # é¢„å¤„ç†å‚è€ƒåˆ†å­
        ref_df = self._preprocess_molecules_parallel(ref_df, "å‚è€ƒåˆ†å­")

        # æµå¼å¤„ç†åº“æ–‡ä»¶
        def library_stream():
            """åº“æ–‡ä»¶æµå¼ç”Ÿæˆå™¨"""
            chunk_size = self.config.get("performance", {}).get("chunk_size", 10000)

            for chunk in pd.read_csv(self.config["data"]["library_csv"], chunksize=chunk_size):
                if not required_cols.issubset(set(chunk.columns)):
                    continue

                # é¢„å¤„ç†chunk
                processed_chunk = self._preprocess_molecules_parallel(chunk, f"åº“åˆ†å­å—")
                if len(processed_chunk) > 0:
                    yield processed_chunk

        self.perf_stats['preprocessing_time'] = time.time() - start_time
        return ref_df, library_stream()

    def load_and_preprocess_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """æ ‡å‡†æ•°æ®åŠ è½½ï¼ˆå‘åå…¼å®¹ï¼‰"""
        print("[INFO] åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰...")
        start_time = time.time()

        # è¯»å–æ•°æ®
        ref_df = pd.read_csv(self.config["data"]["references_csv"])
        lib_df = pd.read_csv(self.config["data"]["library_csv"])

        print(f"[INFO] åŸå§‹æ•°æ®: å‚è€ƒåˆ†å­={len(ref_df)}, åº“åˆ†å­={len(lib_df)}")

        # éªŒè¯å¿…éœ€åˆ—
        required_cols = {"id", "name", "smiles"}
        for df, name in [(ref_df, "references"), (lib_df, "library")]:
            if not required_cols.issubset(set(df.columns)):
                raise ValueError(f"{name} ç¼ºå°‘å¿…éœ€åˆ—: {required_cols - set(df.columns)}")

        # å¹¶è¡Œåˆ†å­é¢„å¤„ç†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        ref_df = self._preprocess_molecules_parallel_optimized(ref_df, "å‚è€ƒåˆ†å­")
        lib_df = self._preprocess_molecules_parallel_optimized(lib_df, "åº“åˆ†å­")

        if len(ref_df) == 0 or len(lib_df) == 0:
            raise ValueError("é¢„å¤„ç†åæ²¡æœ‰æœ‰æ•ˆåˆ†å­")

        print(f"[INFO] æœ‰æ•ˆåˆ†å­: å‚è€ƒ={len(ref_df)}, åº“={len(lib_df)}")
        self.perf_stats['preprocessing_time'] = time.time() - start_time
        return ref_df, lib_df

    def _preprocess_molecules_parallel_optimized(self, df: pd.DataFrame, desc: str) -> pd.DataFrame:
        """ä¼˜åŒ–çš„å¹¶è¡Œåˆ†å­é¢„å¤„ç†"""
        print(f"[INFO] é¢„å¤„ç†{desc}ï¼ˆä¼˜åŒ–ç‰ˆï¼‰...")

        # å‡†å¤‡æ•°æ®
        smiles_data = [(idx, row['smiles'], row) for idx, row in df.iterrows()]

        # ä¼˜åŒ–çš„å¹¶è¡Œæ ‡å‡†åŒ– - ä½¿ç”¨æ›´å¤§çš„æ‰¹å¤„ç†
        batch_size = max(100, len(smiles_data) // (self.cpu_pool.max_workers * 2))

        standardize_func = partial(self._standardize_molecule_with_data_cached)
        valid_mols = self.cpu_pool.map_parallel(
            standardize_func,
            smiles_data,
            desc=f"æ ‡å‡†åŒ–{desc}",
            use_processes=True,
            batch_size=batch_size
        )

        # è¿‡æ»¤æœ‰æ•ˆç»“æœï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
        valid_mols = [mol for mol in valid_mols if mol is not None]

        if not valid_mols:
            return pd.DataFrame()

        result_df = pd.DataFrame(valid_mols)

        # å»é‡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        before_dedup = len(result_df)
        # ä½¿ç”¨hashåŠ é€Ÿå»é‡
        result_df['smiles_hash'] = result_df['canonical_smiles'].apply(hash)
        result_df = result_df.drop_duplicates(subset=['smiles_hash']).reset_index(drop=True)
        result_df = result_df.drop('smiles_hash', axis=1)
        after_dedup = len(result_df)

        if before_dedup != after_dedup:
            print(f"[INFO] {desc}å»é‡: {before_dedup} â†’ {after_dedup}")

        return result_df

    def _preprocess_molecules_parallel(self, df: pd.DataFrame, desc: str) -> pd.DataFrame:
        """å‘åå…¼å®¹çš„é¢„å¤„ç†æ–¹æ³•"""
        return self._preprocess_molecules_parallel_optimized(df, desc)

    @lru_cache(maxsize=10000)
    def _standardize_molecule_cached(self, smiles: str) -> Optional[str]:
        """ç¼“å­˜çš„åˆ†å­æ ‡å‡†åŒ–"""
        mol = self._standardize_molecule(smiles)
        return Chem.MolToSmiles(mol, isomericSmiles=True) if mol else None

    def _standardize_molecule_with_data_cached(self, data: Tuple) -> Optional[Dict]:
        """ä½¿ç”¨ç¼“å­˜çš„åˆ†å­æ ‡å‡†åŒ–"""
        idx, smiles, row = data

        # æ£€æŸ¥ç¼“å­˜
        canonical_smiles = self._standardize_molecule_cached(smiles)

        if canonical_smiles:
            mol = Chem.MolFromSmiles(canonical_smiles)
            if mol:
                self.perf_stats['cache_hits'] += 1
                new_row = row.to_dict()
                new_row['mol'] = mol
                new_row['canonical_smiles'] = canonical_smiles
                return new_row

        self.perf_stats['cache_misses'] += 1
        return None

    def _standardize_molecule_with_data(self, data: Tuple) -> Optional[Dict]:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self._standardize_molecule_with_data_cached(data)

    def _standardize_molecule(self, smiles: str) -> Optional[Chem.Mol]:
        """ä¼˜åŒ–çš„åˆ†å­æ ‡å‡†åŒ–"""
        if not isinstance(smiles, str) or not smiles.strip():
            return None

        try:
            # åŸºç¡€è§£æ
            mol = Chem.MolFromSmiles(smiles.strip())
            if mol is None:
                return None

            # å¿«é€Ÿæœ‰æ•ˆæ€§æ£€æŸ¥
            if mol.GetNumAtoms() == 0 or mol.GetNumAtoms() > 150:  # åŸå­æ•°è¿‡æ»¤
                return None

            # RDKitæ ‡å‡†åŒ–
            Chem.SanitizeMol(mol)

            # ç®€åŒ–çš„æ ‡å‡†åŒ–æµç¨‹ï¼ˆè·³è¿‡è€—æ—¶çš„æ­¥éª¤ï¼‰
            if rdMS is not None:
                try:
                    # åªè¿›è¡Œå¿…è¦çš„æ ‡å‡†åŒ–
                    normalizer = rdMS.Normalizer()
                    mol = normalizer.normalize(mol)
                except:
                    pass

            # é€šè¿‡canonical SMILESé‡å»º
            canonical_smiles = Chem.MolToSmiles(mol, isomericSmiles=True)
            return Chem.MolFromSmiles(canonical_smiles)

        except Exception:
            return None

    def extract_molecular_features_optimized(self, df: pd.DataFrame, desc: str) -> Dict[str, np.ndarray]:
        """ä¼˜åŒ–çš„åˆ†å­ç‰¹å¾æå–"""
        print(f"[INFO] æå–{desc}ç‰¹å¾ï¼ˆè¶…é«˜é€Ÿç‰ˆï¼‰...")
        start_time = time.time()

        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰åˆ†å­æœ‰3Dæ„è±¡
        print(f"[DEBUG] ç¡®ä¿åˆ†å­æœ‰3Dæ„è±¡...")
        molecules = []
        for mol in df['mol'].tolist():
            if mol is not None:
                mol_3d = self._ensure_mol_has_conformer(mol)
                molecules.append(mol_3d)
            else:
                molecules.append(None)

        n_mols = len(molecules)

        # é¢„åŠ è½½æ¨¡å‹
        if 'mol2vec_model' not in self._precomputed_models:
            self._precomputed_models['mol2vec_model'] = self._load_mol2vec_model()

        if 'morgan_params' not in self._precomputed_models:
            self._precomputed_models['morgan_params'] = self.config["fingerprints"]

        if 'pharm_factory' not in self._precomputed_models:
            self._precomputed_models['pharm_factory'] = Gobbi_Pharm2D.factory

        # ========== ç›´æ¥æå–MorganæŒ‡çº¹ ==========
        print(f"[DEBUG] ç›´æ¥æå–{desc}çš„MorganæŒ‡çº¹...")
        morgan_fps = []
        for i, mol in enumerate(molecules):
            if mol is not None:
                try:
                    fp = AllChem.GetMorganFingerprintAsBitVect(
                        mol,
                        radius=self.config["fingerprints"]["morgan_radius"],
                        nBits=self.config["fingerprints"]["morgan_nbits"]
                    )
                    fp_array = np.array(fp, dtype=np.uint8)
                    morgan_fps.append(fp_array)

                    if i < 3:
                        print(f"[DEBUG] {desc}[{i}] Morganç½®ä½: {np.sum(fp_array)}")
                except Exception as e:
                    print(f"[WARNING] {desc}[{i}] Morganå¤±è´¥: {e}")
                    morgan_fps.append(None)
            else:
                morgan_fps.append(None)

        valid_morgan = sum(1 for fp in morgan_fps if fp is not None)
        print(f"[INFO] {desc}æˆåŠŸæå– {valid_morgan}/{n_mols} ä¸ªMorganæŒ‡çº¹")

        # ========== ç›´æ¥æå–è¯æ•ˆå›¢æŒ‡çº¹ï¼ˆæ–°å¢ï¼‰==========
        print(f"[DEBUG] ç›´æ¥æå–{desc}çš„è¯æ•ˆå›¢æŒ‡çº¹...")
        pharm_fps = []
        pharm_factory = self._precomputed_models['pharm_factory']

        for i, mol in enumerate(molecules):
            if mol is not None:
                try:
                    # ç”Ÿæˆè¯æ•ˆå›¢æŒ‡çº¹
                    pharm_fp = Generate.Gen2DFingerprint(mol, pharm_factory)
                    pharm_array = np.array(pharm_fp, dtype=np.uint8)
                    pharm_fps.append(pharm_array)

                    # è°ƒè¯•å‰å‡ ä¸ª
                    if i < 3:
                        bit_count = np.sum(pharm_array)
                        print(f"[DEBUG] {desc}[{i}] è¯æ•ˆå›¢ç½®ä½: {bit_count}")
                        print(f"[DEBUG] {desc}[{i}] è¯æ•ˆå›¢å‰10ä½: {pharm_array[:10]}")

                        if bit_count == 0:
                            print(f"[WARNING] {desc}[{i}] è¯æ•ˆå›¢å…¨æ˜¯0ï¼")
                            # è¾“å‡ºåˆ†å­ä¿¡æ¯
                            try:
                                smiles = Chem.MolToSmiles(mol)
                                print(f"[DEBUG] åˆ†å­SMILES: {smiles[:60]}")
                                print(f"[DEBUG] åŸå­æ•°: {mol.GetNumAtoms()}")
                                print(f"[DEBUG] ç¯æ•°: {mol.GetRingInfo().NumRings()}")
                            except Exception as e2:
                                print(f"[DEBUG] æ— æ³•è·å–åˆ†å­ä¿¡æ¯: {e2}")

                except Exception as e:
                    print(f"[WARNING] {desc}[{i}] è¯æ•ˆå›¢å¤±è´¥: {e}")
                    pharm_fps.append(None)
            else:
                pharm_fps.append(None)

        valid_pharm = sum(1 for fp in pharm_fps if fp is not None)
        print(f"[INFO] {desc}æˆåŠŸæå– {valid_pharm}/{n_mols} ä¸ªè¯æ•ˆå›¢æŒ‡çº¹")

        # æ£€æŸ¥è¯æ•ˆå›¢æŒ‡çº¹æ˜¯å¦å…¨æ˜¯0
        non_zero_pharm = sum(1 for fp in pharm_fps if fp is not None and np.sum(fp) > 0)
        print(f"[INFO] {desc}æœ‰ {non_zero_pharm}/{valid_pharm} ä¸ªéé›¶è¯æ•ˆå›¢æŒ‡çº¹")

        if non_zero_pharm == 0:
            print(f"[ERROR] {desc}æ‰€æœ‰è¯æ•ˆå›¢æŒ‡çº¹éƒ½æ˜¯0ï¼")
        # ============================================

        # æ‰¹é‡ç‰¹å¾æå–ï¼ˆå…¶ä»–ç‰¹å¾ï¼‰
        batch_size = max(500, n_mols // (self.cpu_pool.max_workers))

        extract_func = partial(
            self._extract_batch_features_cached,
            morgan_params=self._precomputed_models['morgan_params'],
            pharm_factory=self._precomputed_models['pharm_factory'],
            mol2vec_model=self._precomputed_models['mol2vec_model']
        )

        # åˆ†æ‰¹å¤„ç†
        all_features = []
        for i in range(0, n_mols, batch_size):
            batch_mols = molecules[i:i + batch_size]
            batch_features = self.cpu_pool.map_parallel(
                extract_func,
                [batch_mols],
                desc=f"{desc}ç‰¹å¾æå– æ‰¹æ¬¡{i // batch_size + 1}",
                use_processes=False
            )
            if batch_features:
                all_features.extend(batch_features[0])

        # æ•´ç†ç‰¹å¾
        features = {
            'morgan_fps': morgan_fps,  # ä½¿ç”¨ç›´æ¥æå–çš„MorganæŒ‡çº¹
            'pharm2d_fps': pharm_fps,  # ä½¿ç”¨ç›´æ¥æå–çš„è¯æ•ˆå›¢æŒ‡çº¹
            'mol2vec_vecs': [],
            'descriptors': [],
            'molecules': molecules
        }

        # ä»æ‰¹å¤„ç†ç»“æœä¸­æå–å…¶ä»–ç‰¹å¾
        for feat_dict in all_features:
            for key in ['mol2vec_vecs', 'descriptors']:
                features[key].append(feat_dict.get(key))

        # ========== æœ€ç»ˆéªŒè¯ ==========
        print(f"[DEBUG] æœ€ç»ˆéªŒè¯{desc}çš„æŒ‡çº¹:")
        print(f"[DEBUG]   - Morganæœ‰æ•ˆæ•°: {valid_morgan}")
        print(f"[DEBUG]   - è¯æ•ˆå›¢æœ‰æ•ˆæ•°: {valid_pharm}")
        print(f"[DEBUG]   - è¯æ•ˆå›¢éé›¶æ•°: {non_zero_pharm}")
        # ==============================

        # GPUè½¬ç§»
        if GPU_AVAILABLE or TORCH_AVAILABLE:
            print(f"[DEBUG] å¼€å§‹GPUè½¬ç§»{desc}ç‰¹å¾...")
            features = self._transfer_to_gpu_ultra_fast(features)

            # éªŒè¯GPUè½¬ç§»å
            if TORCH_AVAILABLE and torch.is_tensor(features['morgan_fps']):
                print(f"[DEBUG] GPUè½¬ç§»å:")
                print(f"[DEBUG]   - Morganå¼ é‡å½¢çŠ¶: {features['morgan_fps'].shape}")
                print(f"[DEBUG]   - Morganå¼ é‡æ€»å’Œ: {torch.sum(features['morgan_fps']).item()}")

                if torch.is_tensor(features['pharm2d_fps']):
                    print(f"[DEBUG]   - è¯æ•ˆå›¢å¼ é‡å½¢çŠ¶: {features['pharm2d_fps'].shape}")
                    pharm_sum = torch.sum(features['pharm2d_fps']).item()
                    print(f"[DEBUG]   - è¯æ•ˆå›¢å¼ é‡æ€»å’Œ: {pharm_sum}")

                    if pharm_sum < 1:
                        print(f"[ERROR] è¯æ•ˆå›¢GPUå¼ é‡å…¨æ˜¯0ï¼")
                elif isinstance(features['pharm2d_fps'], list):
                    print(f"[WARNING] è¯æ•ˆå›¢ä»ç„¶æ˜¯åˆ—è¡¨ï¼Œæœªè½¬æ¢ä¸ºå¼ é‡")

        self.perf_stats['feature_extraction_time'] += time.time() - start_time
        return features

    # def extract_molecular_features_optimized(self, df: pd.DataFrame, desc: str) -> Dict[str, np.ndarray]:
    #     """æç®€ç‰ˆç‰¹å¾æå– - ç”¨äºå¿«é€Ÿæµ‹è¯•"""
    #     print(f"[INFO] {desc}æç®€ç‰ˆç‰¹å¾æå–...")
    #     start_time = time.time()
    #
    #     molecules = df['mol'].tolist()
    #     n_mols = len(molecules)
    #
    #     print(f"[DEBUG] å¤„ç† {n_mols} ä¸ªåˆ†å­")
    #
    #     # åªæå–æœ€åŸºæœ¬çš„MorganæŒ‡çº¹
    #     features = {
    #         'morgan_fps': [],
    #         'pharm2d_fps': [None] * n_mols,  # å…¨éƒ¨è®¾ä¸ºNoneï¼Œè·³è¿‡è¯æ•ˆå›¢è®¡ç®—
    #         'mol2vec_vecs': [None] * n_mols,  # å…¨éƒ¨è®¾ä¸ºNoneï¼Œè·³è¿‡mol2vecè®¡ç®—
    #         'descriptors': [],  # åªè®¡ç®—åŸºæœ¬æè¿°ç¬¦
    #         'molecules': molecules
    #     }
    #
    #     # å›ºå®šå‚æ•°ï¼Œé¿å…ä»é…ç½®è¯»å–
    #     morgan_radius = 2
    #     morgan_nbits = 1024  # å‡å°å°ºå¯¸æé«˜é€Ÿåº¦
    #
    #     print("[DEBUG] å¼€å§‹å¤„ç†MorganæŒ‡çº¹å’ŒåŸºæœ¬æè¿°ç¬¦...")
    #
    #     for i, mol in enumerate(molecules):
    #         # è¿›åº¦æ˜¾ç¤º
    #         if i % 500 == 0 and i > 0:
    #             progress = (i / n_mols) * 100
    #             elapsed = time.time() - start_time
    #             speed = i / elapsed if elapsed > 0 else 0
    #             print(f"[DEBUG] è¿›åº¦: {i}/{n_mols} ({progress:.1f}%) - é€Ÿåº¦: {speed:.1f} åˆ†å­/ç§’")
    #
    #         if mol is not None:
    #             try:
    #                 # MorganæŒ‡çº¹
    #                 fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(
    #                     mol,
    #                     radius=morgan_radius,
    #                     nBits=morgan_nbits
    #                 )
    #                 features['morgan_fps'].append(np.array(fp, dtype=np.uint8))
    #
    #                 # åŸºæœ¬æè¿°ç¬¦ï¼ˆåªè®¡ç®—æœ€é‡è¦çš„å‡ ä¸ªï¼‰
    #                 try:
    #                     desc_dict = {
    #                         'mw': float(Descriptors.MolWt(mol)),
    #                         'logp': float(Crippen.MolLogP(mol)),
    #                         'hbd': float(Lipinski.NumHDonors(mol)),
    #                         'hba': float(Lipinski.NumHAcceptors(mol)),
    #                     }
    #                     features['descriptors'].append(desc_dict)
    #                 except Exception as e:
    #                     print(f"[WARNING] åˆ†å­ {i} æè¿°ç¬¦è®¡ç®—å¤±è´¥: {e}")
    #                     features['descriptors'].append({
    #                         'mw': 0.0, 'logp': 0.0, 'hbd': 0.0, 'hba': 0.0
    #                     })
    #
    #             except Exception as e:
    #                 print(f"[WARNING] åˆ†å­ {i} MorganæŒ‡çº¹è®¡ç®—å¤±è´¥: {e}")
    #                 features['morgan_fps'].append(None)
    #                 features['descriptors'].append({
    #                     'mw': 0.0, 'logp': 0.0, 'hbd': 0.0, 'hba': 0.0
    #                 })
    #         else:
    #             # å¤„ç†ç©ºåˆ†å­
    #             features['morgan_fps'].append(None)
    #             features['descriptors'].append({
    #                 'mw': 0.0, 'logp': 0.0, 'hbd': 0.0, 'hba': 0.0
    #             })
    #
    #     elapsed_time = time.time() - start_time
    #     print(f"[INFO] {desc}æç®€ç‰ˆå®Œæˆ - è€—æ—¶: {elapsed_time:.2f}ç§’")
    #     print(f"[INFO] å¤„ç†é€Ÿåº¦: {n_mols / elapsed_time:.1f} åˆ†å­/ç§’")
    #
    #     # ä¸è¿›è¡ŒGPUè½¬ç§»ï¼Œç›´æ¥è¿”å›CPUç‰ˆæœ¬
    #     return features



    def extract_molecular_features(self, df: pd.DataFrame, desc: str) -> Dict[str, np.ndarray]:
        """å‘åå…¼å®¹çš„ç‰¹å¾æå–æ–¹æ³•"""
        return self.extract_molecular_features_optimized(df, desc)

    def _extract_batch_features_cached(self, molecules_batch: List[Chem.Mol],
                                       morgan_params: dict, pharm_factory, mol2vec_model) -> List[Dict]:
        """æ‰¹é‡ç‰¹å¾æå–ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
        results = []

        for mol in molecules_batch:
            # æ£€æŸ¥ç¼“å­˜
            cached_features = {}
            cache_keys = ['morgan_fps', 'pharm2d_fps', 'mol2vec_vecs', 'descriptors']

            for feature_type in cache_keys:
                cached = self.feature_cache.get(mol, feature_type)
                if cached is not None:
                    cached_features[feature_type] = cached

            # è®¡ç®—ç¼ºå¤±çš„ç‰¹å¾
            if len(cached_features) == len(cache_keys):
                # å…¨éƒ¨å‘½ä¸­ç¼“å­˜
                results.append(cached_features)
                self.perf_stats['cache_hits'] += 1
            else:
                # è®¡ç®—ç‰¹å¾
                features = self._extract_single_molecule_features_fast(
                    mol, morgan_params, pharm_factory, mol2vec_model
                )

                # æ›´æ–°ç¼“å­˜
                for feature_type, feature_value in features.items():
                    if feature_type in cache_keys:
                        self.feature_cache.set(mol, feature_type, feature_value)

                results.append(features)
                self.perf_stats['cache_misses'] += 1

        return results

    def _ensure_mol_has_conformer(self, mol: Chem.Mol) -> Chem.Mol:
        """ç¡®ä¿åˆ†å­æœ‰3Dæ„è±¡ï¼ˆç”¨äºè¯æ•ˆå›¢è®¡ç®—ï¼‰"""
        if mol is None:
            return None

        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ„è±¡
            if mol.GetNumConformers() > 0:
                return mol

            # æ·»åŠ æ°¢åŸå­ï¼ˆè¯æ•ˆå›¢éœ€è¦ï¼‰
            mol_h = Chem.AddHs(mol)

            # ç”Ÿæˆ3Dæ„è±¡
            params = AllChem.ETKDGv3()
            params.randomSeed = 42
            params.useRandomCoords = True

            embed_result = AllChem.EmbedMolecule(mol_h, params)

            if embed_result == -1:
                print(f"[WARNING] 3Dæ„è±¡ç”Ÿæˆå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨2Dåæ ‡")
                # å›é€€åˆ°2Dåæ ‡
                AllChem.Compute2DCoords(mol_h)
                return mol_h

            # å¿«é€Ÿä¼˜åŒ–
            AllChem.MMFFOptimizeMolecule(mol_h, maxIters=50)

            return mol_h

        except Exception as e:
            print(f"[WARNING] æ„è±¡ç”Ÿæˆå¤±è´¥: {e}")
            return mol
    def _extract_single_molecule_features_fast(self, mol: Chem.Mol, morgan_params: dict,
                                               pharm_factory, mol2vec_model) -> Dict:
        """å•åˆ†å­ç‰¹å¾æå– - ä¿®å¤è¯æ•ˆå›¢æŒ‡çº¹"""
        result = {}

        if mol is None:
            return {
                'morgan_fps': None,
                'pharm2d_fps': None,
                'mol2vec_vecs': None,
                'descriptors': {}
            }

        try:
            # MorganæŒ‡çº¹ï¼ˆå·²åœ¨ä¸»å‡½æ•°ä¸­å¤„ç†ï¼‰
            result['morgan_fps'] = None

            # Mol2vec
            if mol2vec_model and MOL2VEC_AVAILABLE:
                mol2vec_vec = self._compute_mol2vec_features_fast(mol, mol2vec_model)
                result['mol2vec_vecs'] = mol2vec_vec
            else:
                result['mol2vec_vecs'] = None

            # ========== ä¿®å¤ï¼šè¯æ•ˆå›¢æŒ‡çº¹ï¼ˆå…³é”®ï¼ï¼‰==========
            try:
                if pharm_factory is not None:
                    # ğŸ”§ ä¿®å¤1ï¼šæ­£ç¡®è½¬æ¢ç¨€ç–å‘é‡ä¸ºå¯†é›†æ•°ç»„
                    pharm_fp = Generate.Gen2DFingerprint(mol, pharm_factory)

                    # æ–¹æ³•1ï¼šä½¿ç”¨ToBitStringï¼ˆæ¨èï¼Œé€‚ç”¨äºå¤§å¤šæ•°æƒ…å†µï¼‰
                    fp_bits = pharm_fp.ToBitString()  # è¿”å› '0101010...' å­—ç¬¦ä¸²
                    pharm_array = np.array([int(bit) for bit in fp_bits], dtype=np.uint8)

                    # æ–¹æ³•2ï¼šä½¿ç”¨GetOnBitsï¼ˆå¤‡é€‰ï¼Œå¦‚æœæ–¹æ³•1å¤±è´¥ï¼‰
                    # pharm_size = pharm_fp.GetNumBits()  # è·å–æŒ‡çº¹é•¿åº¦
                    # pharm_array = np.zeros(pharm_size, dtype=np.uint8)
                    # on_bits = pharm_fp.GetOnBits()  # è·å–ç½®ä½çš„æ¯”ç‰¹ä½ç½®
                    # pharm_array[list(on_bits)] = 1

                    result['pharm2d_fps'] = pharm_array

                    # éªŒè¯
                    bit_count = np.sum(pharm_array)
                    if bit_count == 0:
                        print(f"[WARNING] è¯æ•ˆå›¢æŒ‡çº¹å…¨æ˜¯0ï¼ˆåˆ†å­å¯èƒ½ç¼ºå°‘è¯æ•ˆå›¢ç‰¹å¾ï¼‰")
                        # æ‰“å°åˆ†å­ä¿¡æ¯ç”¨äºè°ƒè¯•
                        try:
                            smiles = Chem.MolToSmiles(mol)
                            print(f"[DEBUG] åˆ†å­SMILES: {smiles[:60]}")
                            print(f"[DEBUG] åŸå­æ•°: {mol.GetNumAtoms()}")
                            print(f"[DEBUG] ç¯æ•°: {mol.GetRingInfo().NumRings()}")
                        except Exception as e2:
                            print(f"[DEBUG] æ— æ³•è·å–åˆ†å­ä¿¡æ¯: {e2}")
                else:
                    print(f"[ERROR] pharm_factory is None!")
                    result['pharm2d_fps'] = None

            except Exception as e:
                print(f"[ERROR] è¯æ•ˆå›¢æŒ‡çº¹ç”Ÿæˆå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                result['pharm2d_fps'] = None
            # ==========================================

            # åˆ†å­æè¿°ç¬¦
            descriptors = self._compute_essential_descriptors_fast(mol)
            result['descriptors'] = descriptors

        except Exception as e:
            print(f"[WARNING] ç‰¹å¾æå–å¤±è´¥: {e}")
            result = {
                'morgan_fps': None,
                'pharm2d_fps': None,
                'mol2vec_vecs': None,
                'descriptors': {}
            }

        return result

    def _extract_single_molecule_features(self, mol: Chem.Mol, morgan_params: dict,
                                          pharm_factory, mol2vec_model) -> Dict:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self._extract_single_molecule_features_fast(mol, morgan_params, pharm_factory, mol2vec_model)

    @lru_cache(maxsize=5000)
    def _compute_morgan_fingerprint_fast(self, mol: Chem.Mol, params_tuple) -> Optional[np.ndarray]:
        """ç¼“å­˜çš„å¿«é€ŸMorganæŒ‡çº¹è®¡ç®—"""
        if mol is None:
            return None

        try:
            # å°†paramsè½¬æ¢ä¸ºå¯å“ˆå¸Œçš„å…ƒç»„ç”¨äºç¼“å­˜
            if isinstance(params_tuple, dict):
                radius = params_tuple["morgan_radius"]
                nbits = params_tuple["morgan_nbits"]
            else:
                radius, nbits = params_tuple

            fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nbits)
            return np.array(fp, dtype=np.uint8)  # ä½¿ç”¨æ›´å°çš„æ•°æ®ç±»å‹
        except Exception:
            return None

    def _compute_morgan_fingerprint(self, mol: Chem.Mol, params: dict) -> Optional[np.ndarray]:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self._compute_morgan_fingerprint_fast(mol, (params["morgan_radius"], params["morgan_nbits"]))

    def _compute_mol2vec_features_fast(self, mol: Chem.Mol, model: object) -> Optional[np.ndarray]:
        """ä¼˜åŒ–çš„Mol2vecç‰¹å¾è®¡ç®—"""
        if mol is None or model is None:
            return None

        try:
            # ä½¿ç”¨æ›´å°çš„radiusä»¥åŠ é€Ÿ
            sentence = mol2alt_sentence(mol, radius=1)

            # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„Gensimï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            try:
                vec = sentences2vec([sentence], model.wv, unseen='UNK')
            except AttributeError:
                vec = sentences2vec([sentence], model, unseen='UNK')

            return vec[0].astype(np.float32) if len(vec) > 0 else None  # ä½¿ç”¨float32èŠ‚çœå†…å­˜
        except Exception:
            return None

    def _compute_mol2vec_features(self, mol: Chem.Mol, model: object) -> Optional[np.ndarray]:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self._compute_mol2vec_features_fast(mol, model)

    def _compute_pharmacophore_fingerprint_fast(self, mol: Chem.Mol, factory) -> Optional[np.ndarray]:
        """ä¼˜åŒ–çš„è¯æ•ˆå›¢æŒ‡çº¹è®¡ç®—"""
        if mol is None:
            return None

        try:
            fp = Generate.Gen2DFingerprint(mol, factory)
            return np.array(fp, dtype=np.uint8)  # ä½¿ç”¨æ›´å°çš„æ•°æ®ç±»å‹
        except Exception:
            return np.zeros(2048, dtype=np.uint8)  # è¿”å›é›¶æŒ‡çº¹

    def _compute_pharmacophore_fingerprint(self, mol: Chem.Mol, factory) -> Optional[np.ndarray]:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self._compute_pharmacophore_fingerprint_fast(mol, factory)

    def _compute_essential_descriptors_fast(self, mol: Chem.Mol) -> Dict[str, float]:
        """è®¡ç®—å…³é”®åˆ†å­æè¿°ç¬¦ï¼ˆä¼˜åŒ–ï¼šåªè®¡ç®—å¿…è¦çš„ï¼‰"""
        if mol is None:
            return {}

        try:
            # åªè®¡ç®—æœ€é‡è¦çš„æè¿°ç¬¦ä»¥æé«˜é€Ÿåº¦
            desc = {
                # Lipinskiäº”é¡¹ï¼ˆå¿…é¡»ï¼‰
                'mw': float(Descriptors.MolWt(mol)),
                'logp': float(Crippen.MolLogP(mol)),
                'hbd': float(Lipinski.NumHDonors(mol)),
                'hba': float(Lipinski.NumHAcceptors(mol)),
                'rotb': float(Lipinski.NumRotatableBonds(mol)),

                # å…³é”®æ‹“æ‰‘æè¿°ç¬¦
                'tpsa': float(rdMolDescriptors.CalcTPSA(mol)),
                'heavy_atoms': float(mol.GetNumHeavyAtoms()),
                'aromatic_rings': float(rdMolDescriptors.CalcNumAromaticRings(mol)),

                # è¯ç‰©ç›¸ä¼¼æ€§ï¼ˆå¯èƒ½è€—æ—¶ï¼Œå¯é€‰ï¼‰
                'qed': float(Descriptors.qed(mol)) if hasattr(Descriptors, 'qed') else 0.0,
            }

            return desc

        except Exception:
            return {}

    def _compute_comprehensive_descriptors(self, mol: Chem.Mol) -> Dict[str, float]:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self._compute_essential_descriptors_fast(mol)

    def _load_mol2vec_model(self) -> Optional[object]:
        """åŠ è½½Mol2vecæ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        model_path = self.config.get("mol2vec", {}).get("model_path")
        if not model_path or not os.path.exists(model_path) or not MOL2VEC_AVAILABLE:
            return None

        try:
            model = word2vec.Word2Vec.load(model_path)
            print(f"[INFO] Mol2vecæ¨¡å‹å·²åŠ è½½: {model_path}")
            return model
        except Exception as e:
            print(f"[WARNING] Mol2vecæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None

    def _transfer_to_gpu_ultra_fast(self, features: Dict) -> Dict:
        """è¶…é«˜é€ŸGPUè½¬ç§»ï¼ˆä¼˜åŒ–å†…å­˜ç®¡ç†ï¼‰"""
        if not (GPU_AVAILABLE or TORCH_AVAILABLE):
            return features

        try:
            gpu_features = {}

            for key, value in features.items():
                if key == 'molecules':
                    gpu_features[key] = value
                    continue

                if not isinstance(value, list) or len(value) == 0:
                    gpu_features[key] = value
                    continue

                # é¢„å¤„ç†å’Œå‘é‡åŒ–
                if key == 'descriptors':
                    gpu_features[key] = self._process_descriptors_gpu(value)
                elif key in ['morgan_fps', 'pharm2d_fps']:
                    gpu_features[key] = self._process_fingerprints_gpu(value, key)
                elif key == 'mol2vec_vecs':
                    gpu_features[key] = self._process_mol2vec_gpu(value)
                else:
                    gpu_features[key] = value

            return gpu_features

        except Exception as e:
            print(f"[WARNING] è¶…é«˜é€ŸGPUè½¬ç§»å¤±è´¥ï¼Œä½¿ç”¨CPU: {e}")
            return features

    def _transfer_to_gpu_optimized(self, features: Dict) -> Dict:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self._transfer_to_gpu_ultra_fast(features)

    def _process_descriptors_gpu(self, descriptors_list):
        """GPUæè¿°ç¬¦å¤„ç† - ä¿®å¤è¿”å›ç±»å‹é—®é¢˜"""
        try:
            # å¿«é€ŸçŸ©é˜µåŒ–
            matrices = []
            all_keys = set()

            # æ”¶é›†æ‰€æœ‰é”®
            for desc_dict in descriptors_list:
                if isinstance(desc_dict, dict):
                    all_keys.update(desc_dict.keys())

            all_keys = sorted(list(all_keys))

            # æ„å»ºçŸ©é˜µ
            for desc_dict in descriptors_list:
                if isinstance(desc_dict, dict) and desc_dict:
                    row = [float(desc_dict.get(key, 0.0)) for key in all_keys]
                else:
                    row = [0.0] * len(all_keys)
                matrices.append(row)

            if matrices:
                matrix = np.array(matrices, dtype=np.float32)

                if TORCH_AVAILABLE:
                    # ä¿®å¤ï¼šç›´æ¥è¿”å›numpyæ•°ç»„è€Œä¸æ˜¯å¼ é‡ï¼Œé¿å…åç»­åˆ¤æ–­é—®é¢˜
                    tensor = torch.from_numpy(matrix.copy())
                    gpu_tensor = tensor.to(device=DEVICE, dtype=torch.float32, non_blocking=True)
                    # ç«‹å³è½¬å›CPU numpyæ•°ç»„ï¼Œé¿å…å¼ é‡ä¼ æ’­
                    return gpu_tensor.cpu().numpy()
                elif GPU_AVAILABLE:
                    gpu_array = cp.asarray(matrix, dtype=cp.float32)
                    # ç«‹å³è½¬å›CPU numpyæ•°ç»„
                    return cp.asnumpy(gpu_array)
                else:
                    return matrix
            else:
                return descriptors_list

        except Exception as e:
            print(f"[WARNING] GPUæè¿°ç¬¦å¤„ç†å¤±è´¥: {e}")
            return descriptors_list

    def _process_fingerprints_gpu(self, fps_list, fp_type):
        """GPUæŒ‡çº¹å¤„ç† - éªŒè¯ç‰ˆæœ¬"""
        try:
            print(f"[DEBUG] å¤„ç†{fp_type}æŒ‡çº¹åˆ°GPU")

            # æ£€æŸ¥è¾“å…¥
            if not fps_list:
                print(f"[ERROR] {fp_type}æŒ‡çº¹åˆ—è¡¨ä¸ºç©º")
                return fps_list

            valid_fps = []
            fp_length = 2048 if 'morgan' in fp_type.lower() else 39972  # è¯æ•ˆå›¢é»˜è®¤é•¿åº¦

            # å¯»æ‰¾ç¬¬ä¸€ä¸ªæœ‰æ•ˆæŒ‡çº¹æ¥ç¡®å®šé•¿åº¦
            for fp in fps_list:
                if fp is not None and hasattr(fp, '__len__'):
                    fp_array = np.array(fp, dtype=np.float32)
                    if len(fp_array) > 0:
                        fp_length = len(fp_array)
                        print(f"[DEBUG] {fp_type}æŒ‡çº¹é•¿åº¦: {fp_length}")
                        break

            # æ„å»ºçŸ©é˜µ
            for i, fp in enumerate(fps_list):
                if fp is not None and hasattr(fp, '__len__'):
                    fp_array = np.array(fp, dtype=np.float32)
                    valid_fps.append(fp_array)

                    # è°ƒè¯•å‰å‡ ä¸ª
                    if i < 3:
                        print(f"[DEBUG] {fp_type}[{i}] éé›¶æ•°: {np.count_nonzero(fp_array)}")
                else:
                    valid_fps.append(np.zeros(fp_length, dtype=np.float32))

            if valid_fps:
                matrix = np.array(valid_fps, dtype=np.float32)

                # è¯¦ç»†éªŒè¯
                print(f"[DEBUG] {fp_type}çŸ©é˜µå½¢çŠ¶: {matrix.shape}")
                print(f"[DEBUG] {fp_type}çŸ©é˜µéé›¶æ¯”ä¾‹: {np.count_nonzero(matrix) / matrix.size * 100:.2f}%")
                print(f"[DEBUG] {fp_type}çŸ©é˜µæ€»å’Œ: {np.sum(matrix)}")

                # æ£€æŸ¥æ˜¯å¦å…¨0
                if np.sum(matrix) < 1:
                    print(f"[ERROR] {fp_type}çŸ©é˜µå…¨æ˜¯0ï¼")
                    # æ‰“å°åŸå§‹åˆ—è¡¨æ ·æœ¬
                    print(f"[DEBUG] åŸå§‹åˆ—è¡¨å‰3ä¸ªæ ·æœ¬:")
                    for i, fp in enumerate(fps_list[:3]):
                        if fp is not None:
                            print(f"  [{i}] type={type(fp)}, sum={np.sum(fp) if hasattr(fp, '__len__') else 'N/A'}")

                if TORCH_AVAILABLE:
                    tensor = torch.from_numpy(matrix.copy())
                    gpu_tensor = tensor.to(device=DEVICE, dtype=torch.float32, non_blocking=True)
                    print(f"[DEBUG] {fp_type}GPUå¼ é‡æ€»å’Œ: {torch.sum(gpu_tensor).item()}")
                    return gpu_tensor
                elif GPU_AVAILABLE:
                    return cp.asarray(matrix, dtype=cp.float32)
                else:
                    return matrix
            else:
                print(f"[ERROR] {fp_type}æ²¡æœ‰æœ‰æ•ˆæŒ‡çº¹")
                return fps_list

        except Exception as e:
            print(f"[ERROR] GPUæŒ‡çº¹å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return fps_list

    def _process_mol2vec_gpu(self, vecs_list):
        """GPU Mol2vecå¤„ç† - ä¿®å¤å†…å­˜å›ºå®šé—®é¢˜"""
        try:
            valid_vecs = []
            vec_length = 300  # é»˜è®¤é•¿åº¦

            for vec in vecs_list:
                if vec is not None and hasattr(vec, '__len__'):
                    vec_array = np.array(vec, dtype=np.float32)
                    if len(vec_array) > 0:
                        vec_length = len(vec_array)
                        break

            # æ„å»ºçŸ©é˜µ
            for vec in vecs_list:
                if vec is not None and hasattr(vec, '__len__'):
                    vec_array = np.array(vec, dtype=np.float32)
                    valid_vecs.append(vec_array)
                else:
                    valid_vecs.append(np.zeros(vec_length, dtype=np.float32))

            if valid_vecs:
                matrix = np.array(valid_vecs, dtype=np.float32)

                if TORCH_AVAILABLE:
                    # ä¿®å¤ï¼šä½¿ç”¨å®‰å…¨çš„è½¬æ¢æ–¹å¼
                    tensor = torch.from_numpy(matrix.copy())
                    return tensor.to(device=DEVICE, dtype=torch.float32, non_blocking=True)
                elif GPU_AVAILABLE:
                    return cp.asarray(matrix, dtype=cp.float32)
                else:
                    return matrix
            else:
                return vecs_list

        except Exception as e:
            print(f"[WARNING] GPU Mol2vecå¤„ç†å¤±è´¥: {e}")
            return vecs_list

    def compute_multi_dimensional_similarity_ultra_fast(self, ref_features: Dict, lib_features: Dict) -> Dict:
        """è¶…é«˜é€Ÿå¤šç»´ç›¸ä¼¼æ€§è®¡ç®— - å®Œå…¨ä¿®å¤ç‰ˆ"""
        print("[INFO] è®¡ç®—å¤šç»´ç›¸ä¼¼æ€§ï¼ˆè¶…é«˜é€Ÿç‰ˆï¼‰...")
        start_time = time.time()

        n_lib = len(lib_features['molecules'])
        n_ref = len(ref_features['molecules'])

        # ========== å®‰å…¨çš„è°ƒè¯•è¾“å‡º ==========
        print(f"\n[DEBUG] ========== 1D MorganæŒ‡çº¹è¯Šæ–­ ==========")
        print(f"[DEBUG] å‚è€ƒåˆ†å­æ•°: {n_ref}")
        print(f"[DEBUG] åº“åˆ†å­æ•°: {n_lib}")

        ref_morgan = ref_features.get('morgan_fps', [])
        lib_morgan = lib_features.get('morgan_fps', [])

        # å®‰å…¨çš„é•¿åº¦æ£€æŸ¥å‡½æ•°
        def safe_len(obj):
            """å®‰å…¨åœ°è·å–å¯¹è±¡é•¿åº¦"""
            try:
                if obj is None:
                    return 0
                if TORCH_AVAILABLE and torch.is_tensor(obj):
                    return obj.shape[0] if len(obj.shape) > 0 else 0
                if hasattr(obj, '__len__'):
                    return len(obj)
                if hasattr(obj, 'shape'):
                    return obj.shape[0]
                return 0
            except:
                return 0

        print(f"[DEBUG] å‚è€ƒMorganæŒ‡çº¹åˆ—è¡¨é•¿åº¦: {safe_len(ref_morgan)}")
        print(f"[DEBUG] åº“MorganæŒ‡çº¹åˆ—è¡¨é•¿åº¦: {safe_len(lib_morgan)}")

        # å®‰å…¨åœ°æ£€æŸ¥ç¬¬ä¸€ä¸ªæŒ‡çº¹
        def safe_check_first(fps, name):
            """å®‰å…¨åœ°æ£€æŸ¥ç¬¬ä¸€ä¸ªæŒ‡çº¹"""
            try:
                if fps is None:
                    print(f"[DEBUG] {name}ä¸ºNone")
                    return

                # PyTorchå¼ é‡
                if TORCH_AVAILABLE and torch.is_tensor(fps):
                    if fps.numel() > 0:
                        print(f"[DEBUG] {name}ç±»å‹: PyTorch Tensor, å½¢çŠ¶: {fps.shape}")
                        return
                    else:
                        print(f"[DEBUG] {name}ä¸ºç©ºå¼ é‡")
                        return

                # åˆ—è¡¨/æ•°ç»„
                length = safe_len(fps)
                if length > 0:
                    print(f"[DEBUG] {name}ç±»å‹: {type(fps)}, é•¿åº¦: {length}")
                    # å°è¯•è·å–ç¬¬ä¸€ä¸ªå…ƒç´ çš„ä¿¡æ¯
                    try:
                        if hasattr(fps, '__getitem__'):
                            first = fps[0]
                            if first is not None:
                                print(f"[DEBUG] {name}ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(first)}")
                    except:
                        pass
                else:
                    print(f"[DEBUG] {name}ä¸ºç©º")
            except Exception as e:
                print(f"[DEBUG] {name}æ£€æŸ¥å¤±è´¥: {e}")

        safe_check_first(ref_morgan, "å‚è€ƒMorganæŒ‡çº¹")
        safe_check_first(lib_morgan, "åº“MorganæŒ‡çº¹")

        print(f"[DEBUG] ==========================================\n")
        # ==========================================

        # å¼‚æ­¥è®¡ç®—ä¸åŒç»´åº¦çš„ç›¸ä¼¼æ€§
        similarity_futures = {}

        with ThreadPoolExecutor(max_workers=3) as executor:
            # 1Dç›¸ä¼¼æ€§
            if self._has_valid_features(ref_features, 'mol2vec_vecs'):
                print("[DEBUG] ä½¿ç”¨mol2vecè®¡ç®—1Dç›¸ä¼¼æ€§")
                similarity_futures['1d'] = executor.submit(
                    self._compute_mol2vec_similarity_ultra_fast,
                    ref_features['mol2vec_vecs'],
                    lib_features['mol2vec_vecs']
                )
            else:
                print("[DEBUG] ä½¿ç”¨MorganæŒ‡çº¹è®¡ç®—1Dç›¸ä¼¼æ€§")
                similarity_futures['1d'] = executor.submit(
                    self._compute_fingerprint_similarity_ultra_fast,
                    ref_features['morgan_fps'],
                    lib_features['morgan_fps']
                )

            # 2Dç›¸ä¼¼æ€§
            similarity_futures['2d'] = executor.submit(
                self._compute_fingerprint_similarity_ultra_fast,
                ref_features['pharm2d_fps'],
                lib_features['pharm2d_fps']
            )

            # 3Dç›¸ä¼¼æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config["similarity"]["w_3d"] > 0:
                similarity_futures['3d'] = executor.submit(
                    self._compute_3d_similarity_fast,
                    ref_features,
                    lib_features
                )
            else:
                similarity_futures['3d'] = executor.submit(
                    lambda: np.zeros((n_lib, n_ref), dtype=np.float32)
                )

        # æ”¶é›†ç»“æœ
        similarities = {}
        for dim, future in similarity_futures.items():
            try:
                similarities[f'{dim}_similarity'] = future.result()
            except Exception as e:
                print(f"[WARNING] {dim}Dç›¸ä¼¼æ€§è®¡ç®—å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                similarities[f'{dim}_similarity'] = np.zeros((n_lib, n_ref), dtype=np.float32)

        self.perf_stats['similarity_computation_time'] = time.time() - start_time
        return similarities

    # def compute_multi_dimensional_similarity_ultra_fast(self, ref_features: Dict, lib_features: Dict) -> Dict:
    #     """æç®€ç‰ˆå¤šç»´ç›¸ä¼¼æ€§è®¡ç®— - åªä½¿ç”¨MorganæŒ‡çº¹"""
    #     print("[INFO] è®¡ç®—ç›¸ä¼¼æ€§ï¼ˆæç®€ç‰ˆ - ä»…MorganæŒ‡çº¹ï¼‰...")
    #     start_time = time.time()
    #
    #     n_lib = len(lib_features['molecules'])
    #     n_ref = len(ref_features['molecules'])
    #
    #     print(f"[DEBUG] åº“åˆ†å­: {n_lib}, å‚è€ƒåˆ†å­: {n_ref}")
    #
    #     # åªè®¡ç®—MorganæŒ‡çº¹ç›¸ä¼¼æ€§
    #     try:
    #         morgan_similarity = self._compute_fingerprint_similarity_minimal(
    #             ref_features['morgan_fps'],
    #             lib_features['morgan_fps']
    #         )
    #
    #         # å…¶ä»–ç›¸ä¼¼æ€§è®¾ä¸ºé›¶
    #         zero_similarity = np.zeros((n_lib, n_ref), dtype=np.float32)
    #
    #         similarities = {
    #             '1d_similarity': morgan_similarity,  # ä½¿ç”¨Morganä½œä¸º1D
    #             '2d_similarity': zero_similarity,  # è·³è¿‡2D
    #             '3d_similarity': zero_similarity  # è·³è¿‡3D
    #         }
    #
    #         elapsed_time = time.time() - start_time
    #         print(f"[INFO] ç›¸ä¼¼æ€§è®¡ç®—å®Œæˆ - è€—æ—¶: {elapsed_time:.2f}ç§’")
    #
    #         return similarities
    #
    #     except Exception as e:
    #         print(f"[ERROR] ç›¸ä¼¼æ€§è®¡ç®—å¤±è´¥: {e}")
    #         # è¿”å›å…¨é›¶çŸ©é˜µ
    #         zero_similarity = np.zeros((n_lib, n_ref), dtype=np.float32)
    #         return {
    #             '1d_similarity': zero_similarity,
    #             '2d_similarity': zero_similarity,
    #             '3d_similarity': zero_similarity
    #         }

    def _compute_fingerprint_similarity_minimal(self, ref_fps: List, lib_fps: List) -> np.ndarray:
        """æç®€ç‰ˆæŒ‡çº¹ç›¸ä¼¼æ€§è®¡ç®— - CPUå‘é‡åŒ–"""
        print("[DEBUG] å¼€å§‹æŒ‡çº¹ç›¸ä¼¼æ€§è®¡ç®—...")

        try:
            # æå–æœ‰æ•ˆæŒ‡çº¹
            valid_ref_fps = []
            valid_lib_fps = []

            for fp in ref_fps:
                if fp is not None:
                    valid_ref_fps.append(np.array(fp, dtype=np.float32))
                else:
                    valid_ref_fps.append(np.zeros(1024, dtype=np.float32))

            for fp in lib_fps:
                if fp is not None:
                    valid_lib_fps.append(np.array(fp, dtype=np.float32))
                else:
                    valid_lib_fps.append(np.zeros(1024, dtype=np.float32))

            if not valid_ref_fps or not valid_lib_fps:
                print("[WARNING] æ²¡æœ‰æœ‰æ•ˆæŒ‡çº¹ï¼Œè¿”å›é›¶çŸ©é˜µ")
                return np.zeros((len(lib_fps), len(ref_fps)), dtype=np.float32)

            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            ref_matrix = np.array(valid_ref_fps, dtype=np.float32)
            lib_matrix = np.array(valid_lib_fps, dtype=np.float32)

            print(f"[DEBUG] å‚è€ƒçŸ©é˜µ: {ref_matrix.shape}, åº“çŸ©é˜µ: {lib_matrix.shape}")

            # ç®€åŒ–çš„Tanimotoç›¸ä¼¼æ€§è®¡ç®—ï¼ˆCPUå‘é‡åŒ–ï¼‰
            print("[DEBUG] è®¡ç®—Tanimotoç›¸ä¼¼æ€§...")

            # ä½¿ç”¨æ‰¹å¤„ç†é¿å…å†…å­˜é—®é¢˜
            batch_size = 1000
            n_lib = lib_matrix.shape[0]
            n_ref = ref_matrix.shape[0]

            similarity = np.zeros((n_lib, n_ref), dtype=np.float32)

            for i in range(0, n_lib, batch_size):
                end_i = min(i + batch_size, n_lib)
                lib_batch = lib_matrix[i:end_i]

                if i % (batch_size * 5) == 0:
                    progress = (i / n_lib) * 100
                    print(f"[DEBUG] ç›¸ä¼¼æ€§è®¡ç®—è¿›åº¦: {progress:.1f}%")

                # è®¡ç®—äº¤é›†
                intersection = np.dot(lib_batch, ref_matrix.T)

                # è®¡ç®—å¹¶é›†
                lib_sum = np.sum(lib_batch, axis=1, keepdims=True)
                ref_sum = np.sum(ref_matrix, axis=1, keepdims=True)
                union = lib_sum + ref_sum.T - intersection

                # Tanimotoç³»æ•°
                batch_similarity = intersection / (union + 1e-8)
                similarity[i:end_i] = batch_similarity

            print("[DEBUG] æŒ‡çº¹ç›¸ä¼¼æ€§è®¡ç®—å®Œæˆ")
            return similarity

        except Exception as e:
            print(f"[ERROR] æŒ‡çº¹ç›¸ä¼¼æ€§è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›é›¶çŸ©é˜µ
            return np.zeros((len(lib_fps), len(ref_fps)), dtype=np.float32)

    def compute_multi_dimensional_similarity(self, ref_features: Dict, lib_features: Dict) -> Dict:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self.compute_multi_dimensional_similarity_ultra_fast(ref_features, lib_features)

    def _has_valid_features(self, features: Dict, feature_type: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç‰¹å¾"""
        if feature_type not in features:
            return False

        feature_list = features[feature_type]
        if not isinstance(feature_list, list):
            return False

        return any(f is not None for f in feature_list)

    def _compute_mol2vec_similarity_ultra_fast(self, ref_vecs, lib_vecs) -> np.ndarray:
        """è¶…é«˜é€ŸMol2vecç›¸ä¼¼æ€§è®¡ç®—"""
        # PyTorch GPUè®¡ç®—ï¼ˆæœ€ä¼˜å…ˆï¼‰
        if TORCH_AVAILABLE and torch.is_tensor(ref_vecs) and torch.is_tensor(lib_vecs):
            try:
                # ä½¿ç”¨PyTorchçš„é«˜åº¦ä¼˜åŒ–çŸ©é˜µè¿ç®—
                with torch.no_grad():
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼æ€§ï¼š(A @ B.T) / (||A|| * ||B||)
                    ref_norm = F.normalize(ref_vecs, p=2, dim=1)
                    lib_norm = F.normalize(lib_vecs, p=2, dim=1)

                    # æ‰¹é‡çŸ©é˜µä¹˜æ³•
                    similarity = torch.mm(lib_norm, ref_norm.T)

                    return similarity.cpu().numpy()

            except Exception as e:
                print(f"[WARNING] PyTorchè¶…é«˜é€Ÿè®¡ç®—å¤±è´¥: {e}")

        # CuPy GPUè®¡ç®—ï¼ˆå¤‡é€‰ï¼‰
        if GPU_AVAILABLE:
            try:
                if not (hasattr(ref_vecs, '__array__') and hasattr(lib_vecs, '__array__')):
                    # è½¬æ¢ä¸ºnumpyæ•°ç»„
                    ref_array = np.array([v for v in ref_vecs if v is not None], dtype=np.float32)
                    lib_array = np.array([v for v in lib_vecs if v is not None], dtype=np.float32)
                else:
                    ref_array = np.asarray(ref_vecs)
                    lib_array = np.asarray(lib_vecs)

                ref_gpu = cp.asarray(ref_array, dtype=cp.float32)
                lib_gpu = cp.asarray(lib_array, dtype=cp.float32)

                # ä½™å¼¦ç›¸ä¼¼æ€§è®¡ç®—
                ref_norm = cp.linalg.norm(ref_gpu, axis=1, keepdims=True)
                lib_norm = cp.linalg.norm(lib_gpu, axis=1, keepdims=True)

                ref_normalized = ref_gpu / (ref_norm + 1e-8)
                lib_normalized = lib_gpu / (lib_norm + 1e-8)

                similarity = cp.dot(lib_normalized, ref_normalized.T)
                return cp.asnumpy(similarity)

            except Exception as e:
                print(f"[WARNING] CuPyè¶…é«˜é€Ÿè®¡ç®—å¤±è´¥: {e}")

        # CPUå‘é‡åŒ–è®¡ç®—ï¼ˆå›é€€ï¼‰
        return self._compute_mol2vec_similarity_cpu_vectorized(ref_vecs, lib_vecs)

    def _compute_mol2vec_similarity_gpu(self, ref_vecs, lib_vecs) -> np.ndarray:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self._compute_mol2vec_similarity_ultra_fast(ref_vecs, lib_vecs)

    def _compute_mol2vec_similarity_cpu_vectorized(self, ref_vecs, lib_vecs) -> np.ndarray:
        """CPUå‘é‡åŒ–Mol2vecç›¸ä¼¼æ€§è®¡ç®—"""
        try:
            # æå–æœ‰æ•ˆå‘é‡
            valid_ref = [v for v in ref_vecs if v is not None]
            valid_lib = [v for v in lib_vecs if v is not None]

            if not valid_ref or not valid_lib:
                return np.zeros((len(lib_vecs), len(ref_vecs)), dtype=np.float32)

            ref_array = np.array(valid_ref, dtype=np.float32)
            lib_array = np.array(valid_lib, dtype=np.float32)

            # ä½¿ç”¨sklearnçš„ä¼˜åŒ–ä½™å¼¦ç›¸ä¼¼æ€§
            similarity = cosine_similarity(lib_array, ref_array).astype(np.float32)

            return similarity

        except Exception as e:
            print(f"[WARNING] CPUå‘é‡åŒ–è®¡ç®—å¤±è´¥: {e}")
            return np.zeros((len(lib_vecs), len(ref_vecs)), dtype=np.float32)

    def _compute_fingerprint_similarity_ultra_fast(self, ref_fps, lib_fps) -> np.ndarray:
        """
        è¶…é«˜é€ŸæŒ‡çº¹ç›¸ä¼¼æ€§è®¡ç®— - å®Œå…¨ä¿®å¤ç‰ˆ
        """
        print(f"[DEBUG] === å¼€å§‹æŒ‡çº¹ç›¸ä¼¼æ€§è®¡ç®— ===")
        print(f"[DEBUG] å‚è€ƒæŒ‡çº¹æ•°é‡: {len(ref_fps) if hasattr(ref_fps, '__len__') else 'unknown'}")
        print(f"[DEBUG] åº“æŒ‡çº¹æ•°é‡: {len(lib_fps) if hasattr(lib_fps, '__len__') else 'unknown'}")

        # ========== å¼ºåˆ¶è½¬æ¢ä¸ºnumpyæ•°ç»„ ==========
        try:
            # å¤„ç†PyTorchå¼ é‡
            if TORCH_AVAILABLE and torch.is_tensor(ref_fps):
                print("[DEBUG] æ£€æµ‹åˆ°PyTorchå¼ é‡ï¼Œè½¬æ¢ä¸ºnumpy")
                ref_fps = ref_fps.cpu().numpy()
                lib_fps = lib_fps.cpu().numpy()

            # å¤„ç†CuPyæ•°ç»„
            elif GPU_AVAILABLE:
                try:
                    import cupy as cp
                    if isinstance(ref_fps, cp.ndarray):
                        print("[DEBUG] æ£€æµ‹åˆ°CuPyæ•°ç»„ï¼Œè½¬æ¢ä¸ºnumpy")
                        ref_fps = cp.asnumpy(ref_fps)
                        lib_fps = cp.asnumpy(lib_fps)
                except:
                    pass

            # å¤„ç†åˆ—è¡¨
            if isinstance(ref_fps, list):
                print("[DEBUG] æ£€æµ‹åˆ°åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºnumpyæ•°ç»„")
                # è¿‡æ»¤Noneå€¼
                valid_ref = []
                for fp in ref_fps:
                    if fp is not None:
                        if hasattr(fp, '__array__'):
                            valid_ref.append(np.array(fp, dtype=np.float32))
                        else:
                            valid_ref.append(fp)

                valid_lib = []
                for fp in lib_fps:
                    if fp is not None:
                        if hasattr(fp, '__array__'):
                            valid_lib.append(np.array(fp, dtype=np.float32))
                        else:
                            valid_lib.append(fp)

                if not valid_ref or not valid_lib:
                    print("[ERROR] è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆæŒ‡çº¹ï¼")
                    return np.zeros((len(lib_fps), len(ref_fps)), dtype=np.float32)

                ref_array = np.array(valid_ref, dtype=np.float32)
                lib_array = np.array(valid_lib, dtype=np.float32)
            else:
                # å·²ç»æ˜¯æ•°ç»„
                ref_array = np.asarray(ref_fps, dtype=np.float32)
                lib_array = np.asarray(lib_fps, dtype=np.float32)

            print(f"[DEBUG] è½¬æ¢åå‚è€ƒæ•°ç»„å½¢çŠ¶: {ref_array.shape}")
            print(f"[DEBUG] è½¬æ¢ååº“æ•°ç»„å½¢çŠ¶: {lib_array.shape}")
            print(f"[DEBUG] å‚è€ƒæ•°ç»„æ•°æ®ç±»å‹: {ref_array.dtype}")
            print(f"[DEBUG] åº“æ•°ç»„æ•°æ®ç±»å‹: {lib_array.dtype}")

            # æ£€æŸ¥æ•°ç»„å†…å®¹
            print(f"[DEBUG] å‚è€ƒæ•°ç»„æ ·æœ¬ï¼ˆå‰5ä¸ªçš„å‰10ä½ï¼‰:")
            for i in range(min(5, len(ref_array))):
                print(f"  Ref {i}: {ref_array[i][:10]}")

            print(f"[DEBUG] åº“æ•°ç»„æ ·æœ¬ï¼ˆå‰5ä¸ªçš„å‰10ä½ï¼‰:")
            for i in range(min(5, len(lib_array))):
                print(f"  Lib {i}: {lib_array[i][:10]}")

            # ========== è®¡ç®—Tanimotoç›¸ä¼¼æ€§ ==========
            print("[DEBUG] å¼€å§‹è®¡ç®—Tanimotoç›¸ä¼¼æ€§...")

            # æ–¹æ³•1ï¼šé€å¯¹è®¡ç®—ï¼ˆæœ€å¯é ï¼‰
            n_lib = lib_array.shape[0]
            n_ref = ref_array.shape[0]
            similarity_matrix = np.zeros((n_lib, n_ref), dtype=np.float32)

            for i in range(n_lib):
                for j in range(n_ref):
                    # Tanimotoç³»æ•° = äº¤é›† / å¹¶é›†
                    intersection = np.sum(np.minimum(lib_array[i], ref_array[j]))
                    union = np.sum(np.maximum(lib_array[i], ref_array[j]))

                    if union > 0:
                        similarity_matrix[i, j] = intersection / union
                    else:
                        similarity_matrix[i, j] = 0.0

            print(f"[DEBUG] ç›¸ä¼¼æ€§çŸ©é˜µå½¢çŠ¶: {similarity_matrix.shape}")
            print(f"[DEBUG] ç›¸ä¼¼æ€§èŒƒå›´: {similarity_matrix.min():.6f} - {similarity_matrix.max():.6f}")
            print(f"[DEBUG] ç›¸ä¼¼æ€§å¹³å‡å€¼: {similarity_matrix.mean():.6f}")
            print(f"[DEBUG] ç›¸ä¼¼æ€§ä¸­ä½æ•°: {np.median(similarity_matrix):.6f}")
            print(f"[DEBUG] éé›¶ç›¸ä¼¼æ€§æ¯”ä¾‹: {(similarity_matrix > 0).sum() / similarity_matrix.size * 100:.2f}%")

            # æ‰“å°ç›¸ä¼¼æ€§çŸ©é˜µçš„ä¸€éƒ¨åˆ†
            print(f"[DEBUG] ç›¸ä¼¼æ€§çŸ©é˜µæ ·æœ¬ï¼ˆå‰5x5ï¼‰:")
            print(similarity_matrix[:5, :5])

            # ========== éªŒè¯è®¡ç®— ==========
            # è‡ªæ£€ï¼šç¬¬ä¸€ä¸ªåº“åˆ†å­ä¸ç¬¬ä¸€ä¸ªå‚è€ƒåˆ†å­çš„ç›¸ä¼¼æ€§
            test_sim = similarity_matrix[0, 0]
            print(f"[DEBUG] éªŒè¯ï¼šLib[0] vs Ref[0] ç›¸ä¼¼æ€§ = {test_sim:.6f}")

            if similarity_matrix.max() == 0:
                print("[ERROR]  æ‰€æœ‰ç›¸ä¼¼æ€§éƒ½æ˜¯0ï¼å¯èƒ½æŒ‡çº¹æ ¼å¼æœ‰é—®é¢˜")
                print(f"[DEBUG] æ£€æŸ¥æŒ‡çº¹å’Œ: Ref[0] sum={ref_array[0].sum()}, Lib[0] sum={lib_array[0].sum()}")
            else:
                print(f"[INFO] ç›¸ä¼¼æ€§è®¡ç®—æˆåŠŸï¼Œæœ€å¤§å€¼={similarity_matrix.max():.6f}")

            return similarity_matrix

        except Exception as e:
            print(f"[ERROR] æŒ‡çº¹ç›¸ä¼¼æ€§è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return np.zeros((len(lib_fps), len(ref_fps)), dtype=np.float32)

    def _compute_fingerprint_similarity_gpu(self, ref_fps, lib_fps) -> np.ndarray:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self._compute_fingerprint_similarity_ultra_fast(ref_fps, lib_fps)

    def _compute_fingerprint_similarity_cpu_vectorized(self, ref_fps, lib_fps) -> np.ndarray:
        """CPUå‘é‡åŒ–æŒ‡çº¹ç›¸ä¼¼æ€§è®¡ç®—"""
        try:
            # æå–æœ‰æ•ˆæŒ‡çº¹
            valid_ref = [fp for fp in ref_fps if fp is not None]
            valid_lib = [fp for fp in lib_fps if fp is not None]

            if not valid_ref or not valid_lib:
                return np.zeros((len(lib_fps), len(ref_fps)), dtype=np.float32)

            ref_array = np.array(valid_ref, dtype=np.float32)
            lib_array = np.array(valid_lib, dtype=np.float32)

            # å‘é‡åŒ–Tanimotoè®¡ç®—
            intersection = np.dot(lib_array, ref_array.T)
            lib_sum = np.sum(lib_array, axis=1, keepdims=True)
            ref_sum = np.sum(ref_array, axis=1, keepdims=True)
            union = lib_sum + ref_sum.T - intersection

            tanimoto = intersection / (union + 1e-8)
            return tanimoto.astype(np.float32)

        except Exception as e:
            print(f"[WARNING] CPUå‘é‡åŒ–æŒ‡çº¹è®¡ç®—å¤±è´¥: {e}")
            return np.zeros((len(lib_fps), len(ref_fps)), dtype=np.float32)

    def _compute_3d_similarity_fast(self, ref_features: Dict, lib_features: Dict) -> np.ndarray:
        """
        å¿«é€Ÿ3Då½¢çŠ¶ç›¸ä¼¼æ€§è®¡ç®— - ä¿®å¤ç‰ˆ
        ç­–ç•¥ï¼š
        1. å…ˆå°è¯•å¿«é€Ÿæ‰¹é‡è®¡ç®—
        2. å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨å¢å¼ºçš„åˆ†å­å¯¹é½æ–¹æ³•
        3. æœ€åå›é€€åˆ°é‡‡æ ·ç­–ç•¥
        """
        if not SHAPE3D_AVAILABLE:
            print("[WARNING] 3Då½¢çŠ¶ç›¸ä¼¼æ€§åº“ä¸å¯ç”¨ï¼Œè¿”å›é›¶çŸ©é˜µ")
            return np.zeros((len(lib_features['molecules']), len(ref_features['molecules'])), dtype=np.float32)

        print("[INFO] è®¡ç®—3Då½¢çŠ¶ç›¸ä¼¼æ€§ï¼ˆå¢å¼ºç‰ˆï¼‰...")

        ref_mols = ref_features['molecules']
        lib_mols = lib_features['molecules']

        n_lib, n_ref = len(lib_mols), len(ref_mols)
        similarity = np.zeros((n_lib, n_ref), dtype=np.float32)

        # ========== ç­–ç•¥1: å¿«é€Ÿæ‰¹é‡è®¡ç®—ï¼ˆä¼˜å…ˆï¼‰ ==========
        try:
            print("[DEBUG] å°è¯•å¿«é€Ÿæ‰¹é‡3Dè®¡ç®—...")

            # é¢„ç”Ÿæˆæ„è±¡ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
            ref_mols_3d = self._batch_generate_conformers(ref_mols, max_conformers=1)
            lib_mols_3d = self._batch_generate_conformers(lib_mols, max_conformers=1)

            # è®¡ç®—æ‰€æœ‰åˆ†å­å¯¹
            computed_count = 0
            for i, lib_mol in enumerate(lib_mols_3d):
                if lib_mol is None:
                    continue

                for j, ref_mol in enumerate(ref_mols_3d):
                    if ref_mol is None:
                        continue

                    try:
                        # ä½¿ç”¨æœ€å¿«çš„å½¢çŠ¶ç›¸ä¼¼æ€§æ–¹æ³•
                        sim = self._compute_fast_shape_similarity(lib_mol, ref_mol)
                        similarity[i, j] = sim
                        computed_count += 1
                    except Exception as e:
                        continue

                # è¿›åº¦æ˜¾ç¤º
                if (i + 1) % 500 == 0:
                    progress = (i + 1) / n_lib * 100
                    print(f"[DEBUG] 3Dè®¡ç®—è¿›åº¦: {progress:.1f}% ({computed_count} å¯¹å®Œæˆ)")

            print(f"[INFO] 3Dç›¸ä¼¼æ€§è®¡ç®—å®Œæˆ: {computed_count}/{n_lib * n_ref} å¯¹")

            # éªŒè¯ç»“æœ
            non_zero_count = np.count_nonzero(similarity)
            print(f"[DEBUG] 3Dç›¸ä¼¼æ€§éé›¶æ•°: {non_zero_count} ({non_zero_count / similarity.size * 100:.2f}%)")
            print(f"[DEBUG] 3Dç›¸ä¼¼æ€§èŒƒå›´: {similarity.min():.4f} - {similarity.max():.4f}")

            if non_zero_count > 0:
                return similarity
            else:
                print("[WARNING] å¿«é€Ÿæ‰¹é‡è®¡ç®—å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")

        except Exception as e:
            print(f"[WARNING] å¿«é€Ÿæ‰¹é‡3Dè®¡ç®—å¤±è´¥: {e}")

        # ========== ç­–ç•¥2: å¢å¼ºçš„å•åˆ†å­å¯¹é½ï¼ˆå¤‡ç”¨ï¼‰ ==========
        try:
            print("[DEBUG] ä½¿ç”¨å¢å¼ºçš„åˆ†å­å¯¹é½æ–¹æ³•...")

            computed_count = 0
            for i in range(n_lib):
                lib_mol = lib_mols[i]
                if lib_mol is None:
                    continue

                for j in range(n_ref):
                    ref_mol = ref_mols[j]
                    if ref_mol is None:
                        continue

                    try:
                        # ä½¿ç”¨æ›´å¯é çš„å¯¹é½æ–¹æ³•
                        sim = self._compute_shape_similarity_robust(lib_mol, ref_mol)
                        similarity[i, j] = sim
                        computed_count += 1
                    except Exception as e:
                        continue

                if (i + 1) % 100 == 0:
                    print(f"[DEBUG] å¢å¼ºå¯¹é½è¿›åº¦: {(i + 1) / n_lib * 100:.1f}%")

            print(f"[INFO] å¢å¼ºå¯¹é½å®Œæˆ: {computed_count} å¯¹")

            if np.count_nonzero(similarity) > 0:
                return similarity

        except Exception as e:
            print(f"[WARNING] å¢å¼ºå¯¹é½å¤±è´¥: {e}")

        # ========== ç­–ç•¥3: é‡‡æ ·ç­–ç•¥ï¼ˆæœ€åæ‰‹æ®µï¼‰ ==========
        print("[WARNING] ä½¿ç”¨é‡‡æ ·ç­–ç•¥ï¼ˆæœ€åæ‰‹æ®µï¼‰...")

        # å¢åŠ é‡‡æ ·æ¯”ä¾‹
        sample_ratio = 0.3  # ä»0.1æå‡åˆ°0.3
        sample_size = max(min(n_lib, n_ref), int(min(n_lib, n_ref) * sample_ratio))

        import random
        lib_indices = random.sample(range(n_lib), min(sample_size, n_lib))
        ref_indices = random.sample(range(n_ref), min(sample_size, n_ref))

        for i in lib_indices:
            for j in ref_indices:
                try:
                    sim = self._compute_shape_similarity_robust(lib_mols[i], ref_mols[j])
                    similarity[i, j] = sim
                except:
                    continue

        print(f"[INFO] é‡‡æ ·è®¡ç®—å®Œæˆ: {len(lib_indices)} x {len(ref_indices)} å¯¹")

        return similarity

    def _batch_generate_conformers(self, mols: List[Chem.Mol], max_conformers: int = 1) -> List[Chem.Mol]:
        """
        æ‰¹é‡ç”Ÿæˆåˆ†å­æ„è±¡
        """
        print(f"[DEBUG] æ‰¹é‡ç”Ÿæˆ {len(mols)} ä¸ªåˆ†å­çš„æ„è±¡...")

        mols_3d = []
        success_count = 0

        for i, mol in enumerate(mols):
            if mol is None:
                mols_3d.append(None)
                continue

            try:
                # æ·»åŠ æ°¢åŸå­
                mol_h = Chem.AddHs(mol)

                # æ£€æŸ¥æ˜¯å¦å·²æœ‰3Dåæ ‡
                if mol_h.GetNumConformers() > 0:
                    mols_3d.append(mol_h)
                    success_count += 1
                    continue

                # ç”Ÿæˆæ„è±¡
                ps = AllChem.ETKDGv3()
                ps.randomSeed = 42
                ps.numThreads = 1
                ps.maxAttempts = 5  # å¿«é€Ÿç”Ÿæˆ

                conf_id = AllChem.EmbedMolecule(mol_h, params=ps)

                if conf_id >= 0:
                    # å¿«é€Ÿä¼˜åŒ–
                    AllChem.MMFFOptimizeMolecule(mol_h, maxIters=50)
                    mols_3d.append(mol_h)
                    success_count += 1
                else:
                    mols_3d.append(None)

            except Exception as e:
                if i < 3:  # åªæ‰“å°å‰å‡ ä¸ªé”™è¯¯
                    print(f"[DEBUG] åˆ†å­{i}æ„è±¡ç”Ÿæˆå¤±è´¥: {e}")
                mols_3d.append(None)

        print(f"[INFO] æ„è±¡ç”ŸæˆæˆåŠŸ: {success_count}/{len(mols)}")
        return mols_3d

    def _compute_fast_shape_similarity(self, mol1: Chem.Mol, mol2: Chem.Mol) -> float:
        """
        å¿«é€Ÿå½¢çŠ¶ç›¸ä¼¼æ€§è®¡ç®—ï¼ˆå•æ„è±¡ï¼‰
        """
        if not mol1 or not mol2:
            return 0.0

        try:
            # æ£€æŸ¥æ„è±¡
            if mol1.GetNumConformers() == 0 or mol2.GetNumConformers() == 0:
                return 0.0

            # ä½¿ç”¨RDKitçš„å¿«é€Ÿå½¢çŠ¶ç›¸ä¼¼æ€§
            dist = rdShapeHelpers.ShapeTanimotoDist(mol1, mol2, 0, 0)
            sim = 1.0 - dist

            return max(0.0, min(1.0, sim))  # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…

        except Exception as e:
            return 0.0

    def _compute_shape_similarity_robust(self, mol1: Chem.Mol, mol2: Chem.Mol,
                                         max_conformers: int = 2) -> float:
        """
        é²æ£’çš„å½¢çŠ¶ç›¸ä¼¼æ€§è®¡ç®—ï¼ˆå¤šæ„è±¡ï¼‰
        """
        if not mol1 or not mol2:
            return 0.0

        try:
            # æ·»åŠ æ°¢åŸå­
            mol1_h = Chem.AddHs(mol1) if mol1.GetNumHeavyAtoms() > 0 else mol1
            mol2_h = Chem.AddHs(mol2) if mol2.GetNumHeavyAtoms() > 0 else mol2

            # ç”Ÿæˆæ„è±¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if mol1_h.GetNumConformers() == 0:
                ps = AllChem.ETKDGv3()
                ps.randomSeed = 42
                ps.numThreads = 1
                AllChem.EmbedMultipleConfs(mol1_h, numConfs=max_conformers, params=ps)

            if mol2_h.GetNumConformers() == 0:
                ps = AllChem.ETKDGv3()
                ps.randomSeed = 42
                ps.numThreads = 1
                AllChem.EmbedMultipleConfs(mol2_h, numConfs=max_conformers, params=ps)

            if mol1_h.GetNumConformers() == 0 or mol2_h.GetNumConformers() == 0:
                return 0.0

            # è®¡ç®—æœ€å¤§ç›¸ä¼¼æ€§
            max_sim = 0.0
            conf_count1 = min(max_conformers, mol1_h.GetNumConformers())
            conf_count2 = min(max_conformers, mol2_h.GetNumConformers())

            for conf1 in range(conf_count1):
                for conf2 in range(conf_count2):
                    try:
                        dist = rdShapeHelpers.ShapeTanimotoDist(mol1_h, mol2_h, conf1, conf2)
                        sim = 1.0 - dist
                        max_sim = max(max_sim, sim)
                    except:
                        continue

            return max(0.0, min(1.0, max_sim))

        except Exception as e:
            return 0.0

    def aggregate_similarities_ultra_fast(self, similarities: Dict) -> Dict:
        """
        ç›¸ä¼¼æ€§èšåˆ - å®Œå…¨ä¸å½’ä¸€åŒ–ç‰ˆæœ¬
        """
        print("[INFO] èšåˆå¤šç»´ç›¸ä¼¼æ€§ï¼ˆæ— å½’ä¸€åŒ–ç‰ˆæœ¬ï¼‰...")

        sim_1d = similarities['1d_similarity']
        sim_2d = similarities['2d_similarity']
        sim_3d = similarities['3d_similarity']

        # è½¬æ¢ä¸ºnumpy
        if torch.is_tensor(sim_1d):
            sim_1d = sim_1d.cpu().numpy()
            sim_2d = sim_2d.cpu().numpy()
            sim_3d = sim_3d.cpu().numpy()
        elif hasattr(sim_1d, '__array__'):
            sim_1d = np.asarray(sim_1d)
            sim_2d = np.asarray(sim_2d)
            sim_3d = np.asarray(sim_3d)

        print(f"[DEBUG] è¾“å…¥ç›¸ä¼¼æ€§çŸ©é˜µå½¢çŠ¶:")
        print(f"  - 1D: {sim_1d.shape}")
        print(f"  - 2D: {sim_2d.shape}")
        print(f"  - 3D: {sim_3d.shape}")

        print(f"[DEBUG] è¾“å…¥ç›¸ä¼¼æ€§èŒƒå›´:")
        print(f"  - 1D: {sim_1d.min():.6f} - {sim_1d.max():.6f}")
        print(f"  - 2D: {sim_2d.min():.6f} - {sim_2d.max():.6f}")
        print(f"  - 3D: {sim_3d.min():.6f} - {sim_3d.max():.6f}")

        # å–æœ€å¤§å€¼ï¼ˆæ¯ä¸ªåº“åˆ†å­ä¸æ‰€æœ‰å‚è€ƒåˆ†å­çš„æœ€å¤§ç›¸ä¼¼æ€§ï¼‰
        max_sim_1d = np.max(sim_1d, axis=1)
        max_sim_2d = np.max(sim_2d, axis=1)
        max_sim_3d = np.max(sim_3d, axis=1)

        print(f"[DEBUG] æœ€å¤§ç›¸ä¼¼æ€§èŒƒå›´:")
        print(f"  - 1D: {max_sim_1d.min():.6f} - {max_sim_1d.max():.6f}")
        print(f"  - 2D: {max_sim_2d.min():.6f} - {max_sim_2d.max():.6f}")
        print(f"  - 3D: {max_sim_3d.min():.6f} - {max_sim_3d.max():.6f}")

        # ========== å®Œå…¨ä¸å½’ä¸€åŒ–ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å€¼ ==========
        w1 = self.config["similarity"]["w_1d"]
        w2 = self.config["similarity"]["w_2d"]
        w3 = self.config["similarity"]["w_3d"]

        print(f"[DEBUG] æƒé‡: w1={w1}, w2={w2}, w3={w3}")

        # ç›´æ¥åŠ æƒï¼Œä¸åšä»»ä½•å½’ä¸€åŒ–
        combined_scores = w1 * max_sim_1d + w2 * max_sim_2d + w3 * max_sim_3d

        print(f"[DEBUG] ç»„åˆå¾—åˆ†èŒƒå›´: {combined_scores.min():.6f} - {combined_scores.max():.6f}")
        print(f"[DEBUG] ç»„åˆå¾—åˆ†å¹³å‡: {combined_scores.mean():.6f}")
        print(f"[DEBUG] ç»„åˆå¾—åˆ†ä¸­ä½æ•°: {np.median(combined_scores):.6f}")

        # ========== è¯¦ç»†å¾—åˆ†åˆ†è§£ï¼ˆå‰10ä¸ªåŒ–åˆç‰©ï¼‰==========
        print(f"[DEBUG] å‰10ä¸ªåŒ–åˆç‰©å¾—åˆ†åˆ†è§£:")
        for i in range(min(10, len(combined_scores))):
            print(f"  [{i}] 1D={max_sim_1d[i]:.4f} * {w1} + "
                  f"2D={max_sim_2d[i]:.4f} * {w2} + "
                  f"3D={max_sim_3d[i]:.4f} * {w3} = "
                  f"{combined_scores[i]:.4f}")

        return {
            'combined_scores': combined_scores,
            'individual_scores': {
                '1d_max': max_sim_1d,
                '2d_max': max_sim_2d,
                '3d_max': max_sim_3d,
                '1d_norm': max_sim_1d,  # è¿”å›åŸå§‹å€¼ï¼Œä¸å½’ä¸€åŒ–
                '2d_norm': max_sim_2d,
                '3d_norm': max_sim_3d
            }
        }

    # def aggregate_similarities_ultra_fast(self, similarities: Dict) -> Dict:
    #     """æç®€ç‰ˆç›¸ä¼¼æ€§èšåˆ"""
    #     print("[INFO] èšåˆç›¸ä¼¼æ€§ï¼ˆæç®€ç‰ˆï¼‰...")
    #
    #     sim_1d = similarities['1d_similarity']  # MorganæŒ‡çº¹ç›¸ä¼¼æ€§
    #     sim_2d = similarities['2d_similarity']  # å…¨é›¶
    #     sim_3d = similarities['3d_similarity']  # å…¨é›¶
    #
    #     # ç®€å•å–æœ€å¤§å€¼ï¼ˆåªä½¿ç”¨MorganæŒ‡çº¹ï¼‰
    #     max_sim_1d = np.max(sim_1d, axis=1)
    #     max_sim_2d = np.zeros_like(max_sim_1d)
    #     max_sim_3d = np.zeros_like(max_sim_1d)
    #
    #     # ç®€å•å½’ä¸€åŒ–
    #     if np.max(max_sim_1d) > 0:
    #         max_sim_1d_norm = max_sim_1d / np.max(max_sim_1d)
    #     else:
    #         max_sim_1d_norm = max_sim_1d
    #
    #     # åªä½¿ç”¨1Dæƒé‡ï¼ˆMorganæŒ‡çº¹ï¼‰
    #     w1 = 1.0  # å…¨éƒ¨æƒé‡ç»™MorganæŒ‡çº¹
    #     combined_scores = w1 * max_sim_1d_norm
    #
    #     return {
    #         'combined_scores': combined_scores,
    #         'individual_scores': {
    #             '1d_max': max_sim_1d,
    #             '2d_max': max_sim_2d,
    #             '3d_max': max_sim_3d,
    #             '1d_norm': max_sim_1d_norm,
    #             '2d_norm': max_sim_2d,
    #             '3d_norm': max_sim_3d
    #         }
    #     }


    def aggregate_similarities(self, similarities: Dict) -> Dict:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self.aggregate_similarities_ultra_fast(similarities)

    def _minmax_normalize_torch(self, tensor: torch.Tensor) -> torch.Tensor:
        min_val = torch.min(tensor)
        max_val = torch.max(tensor)
        if max_val - min_val < 1e-12:
            return tensor.clone()  # ä¿®å¤
        return (tensor - min_val) / (max_val - min_val)

    def _minmax_normalize_cupy(self, array: cp.ndarray) -> cp.ndarray:
        min_val = cp.min(array)
        max_val = cp.max(array)
        if max_val - min_val < 1e-12:
            return array.copy()  #  ä¿®å¤
        return (array - min_val) / (max_val - min_val)

    def _minmax_normalize(self, arr: np.ndarray) -> np.ndarray:
        if len(arr) == 0:
            return arr
        min_val, max_val = np.min(arr), np.max(arr)
        if max_val - min_val < 1e-12:
            return arr.copy()  # ä¿®å¤
        return (arr - min_val) / (max_val - min_val)

    def apply_ai_enhanced_filtering_ultra_fast(self, lib_df: pd.DataFrame, scores: Dict,
                                               lib_features: Dict) -> pd.DataFrame:
        """è¶…é«˜é€ŸAIå¢å¼ºè¿‡æ»¤ - ä¿®å¤èšç±»é”™è¯¯"""
        print("[INFO] åº”ç”¨AIå¢å¼ºè¿‡æ»¤ï¼ˆè¶…é«˜é€Ÿç‰ˆï¼‰...")
        start_time = time.time()

        # åˆ›å»ºç»“æœæ•°æ®æ¡†
        results = lib_df[['id', 'name', 'smiles', 'canonical_smiles']].copy()
        results['combined_score'] = scores['combined_scores']

        # æ·»åŠ ä¸ªåˆ«ç›¸ä¼¼æ€§å¾—åˆ†
        for key, values in scores['individual_scores'].items():
            results[key] = values

        # å¿«é€Ÿæè¿°ç¬¦å¤„ç†
        try:
            descriptors_data = lib_features.get('descriptors', [])
            if descriptors_data is not None:
                descriptors_df = self._process_descriptors_dataframe(descriptors_data)
                if not descriptors_df.empty:
                    for col in descriptors_df.columns:
                        results[col] = descriptors_df[col]
        except Exception as e:
            print(f"[WARNING] æè¿°ç¬¦å¤„ç†å¤±è´¥: {e}")
            descriptors_df = pd.DataFrame()  # ç¡®ä¿å®šä¹‰äº†

        # å¹¶è¡Œè¿‡æ»¤ä»»åŠ¡
        filtering_results = {}

        with ThreadPoolExecutor(max_workers=min(4, self.cpu_pool.max_workers)) as executor:
            futures = {}

            # PAINSè¿‡æ»¤
            if self.config.get("filters", {}).get("pains", False):
                futures['pains'] = executor.submit(
                    self._batch_pains_check,
                    lib_df['mol'].tolist()
                )

            # CNSè¿‡æ»¤
            if self.config.get("filters", {}).get("cns_filter", False):
                default_cns_rules = {
                    "mw_min": 130, "mw_max": 725, "logp_min": -7, "logp_max": 5.5,
                    "hbd_max": 7, "hba_max": 12, "tpsa_max": 200, "rotb_max": 11
                }
                cns_rules = self.config.get("filters", {}).get("cns_rules", default_cns_rules)
                futures['cns'] = executor.submit(
                    self._batch_cns_check,
                    results,
                    cns_rules
                )

            # AIèšç±»ä»»åŠ¡ï¼ˆä¿®å¤ï¼šå®‰å…¨æ£€æŸ¥ï¼‰
            if SKLEARN_AVAILABLE and self.config.get("ai_model", {}).get("use_clustering", False):
                if 'descriptors_df' in locals() and not descriptors_df.empty and len(descriptors_df) > 0:
                    futures['cluster'] = executor.submit(
                        self._perform_clustering_fast,
                        descriptors_df
                    )
                else:
                    print("[WARNING] æè¿°ç¬¦ä¸ºç©ºï¼Œè·³è¿‡èšç±»")
                    filtering_results['cluster'] = None

            # å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆä¿®å¤ï¼šå®‰å…¨æ£€æŸ¥ï¼‰
            if SKLEARN_AVAILABLE and self.config.get("ai_model", {}).get("use_outlier_filter", False):
                if 'descriptors_df' in locals() and not descriptors_df.empty and len(descriptors_df) > 0:
                    futures['outlier'] = executor.submit(
                        self._detect_outliers_fast,
                        descriptors_df
                    )
                else:
                    print("[WARNING] æè¿°ç¬¦ä¸ºç©ºï¼Œè·³è¿‡å¼‚å¸¸å€¼æ£€æµ‹")
                    filtering_results['outlier'] = None

            # æ”¶é›†ç»“æœ
            for task_name, future in futures.items():
                try:
                    filtering_results[task_name] = future.result()
                except Exception as e:
                    print(f"[WARNING] {task_name}ä»»åŠ¡å¤±è´¥: {e}")
                    filtering_results[task_name] = None

        # åº”ç”¨è¿‡æ»¤ç»“æœï¼ˆä¿®å¤ï¼šå®‰å…¨èµ‹å€¼ï¼‰
        results['is_pains'] = filtering_results.get('pains', [False] * len(results))
        results['cns_compliant'] = filtering_results.get('cns', [True] * len(results))

        # ========== ä¿®å¤ï¼šå®‰å…¨çš„èšç±»ç»“æœèµ‹å€¼ ==========
        cluster_result = filtering_results.get('cluster')
        if cluster_result is not None and len(cluster_result) == len(results):
            results['cluster'] = cluster_result
        else:
            # å¦‚æœèšç±»å¤±è´¥æˆ–é•¿åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤å€¼
            results['cluster'] = 0
            print(f"[WARNING] èšç±»ç»“æœæ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼0")

        outlier_result = filtering_results.get('outlier')
        if outlier_result is not None and len(outlier_result) == len(results):
            results['is_outlier'] = outlier_result == -1
        else:
            results['is_outlier'] = False
            print(f"[WARNING] å¼‚å¸¸å€¼æ£€æµ‹ç»“æœæ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼False")
        # =============================================

        # å¿«é€Ÿæ’åº
        sort_indices = np.argsort(-results['combined_score'].values)
        results = results.iloc[sort_indices].reset_index(drop=True)

        self.perf_stats['filtering_time'] = time.time() - start_time
        return results

    # def apply_ai_enhanced_filtering_ultra_fast(self, lib_df: pd.DataFrame, scores: Dict,
    #                                            lib_features: Dict) -> pd.DataFrame:
    #     """æç®€ç‰ˆAIå¢å¼ºè¿‡æ»¤"""
    #     print("[INFO] åº”ç”¨è¿‡æ»¤ï¼ˆæç®€ç‰ˆï¼‰...")
    #     start_time = time.time()
    #
    #     # åˆ›å»ºç»“æœæ•°æ®æ¡†
    #     results = lib_df[['id', 'name', 'smiles', 'canonical_smiles']].copy()
    #     results['combined_score'] = scores['combined_scores']
    #
    #     # æ·»åŠ ä¸ªåˆ«ç›¸ä¼¼æ€§å¾—åˆ†
    #     for key, values in scores['individual_scores'].items():
    #         results[key] = values
    #
    #     # æ·»åŠ åŸºæœ¬æè¿°ç¬¦
    #     descriptors_list = lib_features['descriptors']
    #     for i, desc_dict in enumerate(descriptors_list):
    #         for key, value in desc_dict.items():
    #             if key not in results.columns:
    #                 results[key] = 0.0
    #             results.loc[i, key] = value
    #
    #     # ç®€åŒ–è¿‡æ»¤ï¼ˆè·³è¿‡PAINSå’Œå¤æ‚AIåŠŸèƒ½ï¼‰
    #     results['is_pains'] = False  # æš‚æ—¶è·³è¿‡PAINSæ£€æŸ¥
    #     results['cns_compliant'] = True  # æš‚æ—¶è·³è¿‡CNSæ£€æŸ¥
    #
    #     # å¿«é€Ÿæ’åº
    #     results = results.sort_values('combined_score', ascending=False).reset_index(drop=True)
    #
    #     elapsed_time = time.time() - start_time
    #     print(f"[INFO] è¿‡æ»¤å®Œæˆ - è€—æ—¶: {elapsed_time:.2f}ç§’")
    #
    #     return results



    # def apply_ai_enhanced_filtering(self, lib_df: pd.DataFrame, scores: Dict, lib_features: Dict) -> pd.DataFrame:
    #     """å‘åå…¼å®¹æ–¹æ³•"""
    #     return self.apply_ai_enhanced_filtering_ultra_fast(lib_df, scores, lib_features)

    # def _process_descriptors_dataframe(self, descriptors_list: List[Dict]) -> pd.DataFrame:
    #     """å¿«é€Ÿå¤„ç†æè¿°ç¬¦åˆ—è¡¨ä¸ºDataFrame"""
    #     if not descriptors_list:
    #         return pd.DataFrame()
    #
    #     # æ”¶é›†æ‰€æœ‰é”®
    #     all_keys = set()
    #     for desc in descriptors_list:
    #         if isinstance(desc, dict):
    #             all_keys.update(desc.keys())
    #
    #     all_keys = sorted(list(all_keys))
    #
    #     # å‘é‡åŒ–æ„å»ºçŸ©é˜µ
    #     data_matrix = np.zeros((len(descriptors_list), len(all_keys)), dtype=np.float32)
    #
    #     for i, desc in enumerate(descriptors_list):
    #         if isinstance(desc, dict):
    #             for j, key in enumerate(all_keys):
    #                 data_matrix[i, j] = float(desc.get(key, 0.0))
    #
    #     return pd.DataFrame(data_matrix, columns=all_keys)

    def _process_descriptors_dataframe(self, descriptors_list) -> pd.DataFrame:
        """å¿«é€Ÿå¤„ç†æè¿°ç¬¦åˆ—è¡¨ä¸ºDataFrame - ä¿®å¤å¼ é‡åˆ¤æ–­é—®é¢˜"""

        # ä¿®å¤ï¼šå®‰å…¨çš„ç©ºå€¼æ£€æŸ¥
        if descriptors_list is None:
            return pd.DataFrame()

        # æ£€æŸ¥æ˜¯å¦ä¸º PyTorch å¼ é‡
        if TORCH_AVAILABLE and torch.is_tensor(descriptors_list):
            # è½¬æ¢å¼ é‡ä¸º numpy æ•°ç»„å†è½¬ä¸º DataFrame
            if descriptors_list.numel() == 0:  # ç©ºå¼ é‡æ£€æŸ¥
                return pd.DataFrame()

            # å¦‚æœæ˜¯GPUå¼ é‡ï¼Œå…ˆç§»åˆ°CPU
            if descriptors_list.is_cuda:
                descriptors_array = descriptors_list.cpu().numpy()
            else:
                descriptors_array = descriptors_list.numpy()

            # ç”Ÿæˆåˆ—å
            n_features = descriptors_array.shape[1] if len(descriptors_array.shape) > 1 else descriptors_array.shape[0]
            columns = [f'desc_{i}' for i in range(n_features)]

            return pd.DataFrame(descriptors_array, columns=columns)

        # æ£€æŸ¥æ˜¯å¦ä¸º CuPy æ•°ç»„
        if GPU_AVAILABLE and hasattr(descriptors_list, '__array__'):
            try:
                import cupy as cp
                if isinstance(descriptors_list, cp.ndarray):
                    if descriptors_list.size == 0:
                        return pd.DataFrame()

                    descriptors_array = cp.asnumpy(descriptors_list)
                    n_features = descriptors_array.shape[1] if len(descriptors_array.shape) > 1 else \
                    descriptors_array.shape[0]
                    columns = [f'desc_{i}' for i in range(n_features)]

                    return pd.DataFrame(descriptors_array, columns=columns)
            except:
                pass

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ™®é€šåˆ—è¡¨
        if isinstance(descriptors_list, list):
            if len(descriptors_list) == 0:
                return pd.DataFrame()

            # æ”¶é›†æ‰€æœ‰é”®
            all_keys = set()
            for desc in descriptors_list:
                if isinstance(desc, dict):
                    all_keys.update(desc.keys())

            if not all_keys:
                return pd.DataFrame()

            all_keys = sorted(list(all_keys))

            # å‘é‡åŒ–æ„å»ºçŸ©é˜µ
            data_matrix = np.zeros((len(descriptors_list), len(all_keys)), dtype=np.float32)

            for i, desc in enumerate(descriptors_list):
                if isinstance(desc, dict):
                    for j, key in enumerate(all_keys):
                        data_matrix[i, j] = float(desc.get(key, 0.0))

            return pd.DataFrame(data_matrix, columns=all_keys)

        # æ£€æŸ¥æ˜¯å¦ä¸º numpy æ•°ç»„
        if isinstance(descriptors_list, np.ndarray):
            if descriptors_list.size == 0:
                return pd.DataFrame()

            n_features = descriptors_list.shape[1] if len(descriptors_list.shape) > 1 else descriptors_list.shape[0]
            columns = [f'desc_{i}' for i in range(n_features)]

            return pd.DataFrame(descriptors_list, columns=columns)

        # å…¶ä»–æƒ…å†µè¿”å›ç©º DataFrame
        print(f"[WARNING] æœªçŸ¥çš„æè¿°ç¬¦æ•°æ®ç±»å‹: {type(descriptors_list)}")
        return pd.DataFrame()

    def _batch_pains_check(self, molecules: List[Chem.Mol]) -> List[bool]:
        """æ‰¹é‡PAINSæ£€æŸ¥"""
        try:
            params = FilterCatalogParams()
            params.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS_A)
            params.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS_B)
            params.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS_C)
            catalog = FilterCatalog(params)

            # å¹¶è¡Œæ£€æŸ¥
            check_func = lambda mol: catalog.HasMatch(mol) if mol else False
            results = self.cpu_pool.map_parallel(
                check_func,
                molecules,
                desc="PAINSæ‰¹é‡æ£€æŸ¥",
                use_processes=False,
                batch_size=1000
            )

            return results

        except Exception as e:
            print(f"[WARNING] æ‰¹é‡PAINSæ£€æŸ¥å¤±è´¥: {e}")
            return [False] * len(molecules)

    def _check_pains(self, mol: Chem.Mol) -> bool:
        """å‘åå…¼å®¹æ–¹æ³•"""
        if mol is None:
            return False
        try:
            params = FilterCatalogParams()
            params.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS_A)
            params.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS_B)
            params.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS_C)
            catalog = FilterCatalog(params)
            return catalog.HasMatch(mol)
        except:
            return False

    def _batch_cns_check(self, results_df: pd.DataFrame, rules: Dict) -> List[bool]:
        """æ‰¹é‡CNSè§„åˆ™æ£€æŸ¥ï¼ˆå‘é‡åŒ–ï¼‰"""
        try:
            # å‘é‡åŒ–è§„åˆ™æ£€æŸ¥
            conditions = []

            if 'mw' in results_df.columns:
                mw_check = (results_df['mw'] >= rules.get("mw_min", 0)) & \
                           (results_df['mw'] <= rules.get("mw_max", 1000))
                conditions.append(mw_check)

            if 'tpsa' in results_df.columns:
                tpsa_check = results_df['tpsa'] <= rules.get("tpsa_max", 200)
                conditions.append(tpsa_check)

            if 'hbd' in results_df.columns:
                hbd_check = results_df['hbd'] <= rules.get("hbd_max", 10)
                conditions.append(hbd_check)

            if 'hba' in results_df.columns:
                hba_check = results_df['hba'] <= rules.get("hba_max", 15)
                conditions.append(hba_check)

            if 'rotb' in results_df.columns:
                rotb_check = results_df['rotb'] <= rules.get("rotb_max", 20)
                conditions.append(rotb_check)

            if 'logp' in results_df.columns:
                logp_check = (results_df['logp'] >= rules.get("logp_min", -10)) & \
                             (results_df['logp'] <= rules.get("logp_max", 10))
                conditions.append(logp_check)

            # ç»„åˆæ‰€æœ‰æ¡ä»¶
            if conditions:
                combined_check = conditions[0]
                for condition in conditions[1:]:
                    combined_check = combined_check & condition
                return combined_check.tolist()
            else:
                return [True] * len(results_df)

        except Exception as e:
            print(f"[WARNING] æ‰¹é‡CNSæ£€æŸ¥å¤±è´¥: {e}")
            return [True] * len(results_df)

    def _check_cns_compliance_row(self, row: pd.Series, rules: Dict) -> bool:
        """å‘åå…¼å®¹æ–¹æ³•"""
        try:
            return (
                    rules.get("mw_min", 0) <= row.get("mw", 0) <= rules.get("mw_max", 1000) and
                    row.get("tpsa", 0) <= rules.get("tpsa_max", 200) and
                    row.get("hbd", 0) <= rules.get("hbd_max", 10) and
                    row.get("hba", 0) <= rules.get("hba_max", 15) and
                    row.get("rotb", 0) <= rules.get("rotb_max", 20) and
                    rules.get("logp_min", -10) <= row.get("logp", 0) <= rules.get("logp_max", 10)
            )
        except:
            return False

    def _perform_clustering_fast(self, descriptors_df: pd.DataFrame, n_clusters: int = 5) -> np.ndarray:
        """å¿«é€Ÿèšç±»åˆ†æï¼ˆä½¿ç”¨MiniBatchKMeansï¼‰"""
        try:
            # é€‰æ‹©æ•°å€¼åˆ—
            numeric_cols = descriptors_df.select_dtypes(include=[np.number]).columns
            X = descriptors_df[numeric_cols].fillna(0).values

            # æ•°æ®é‡‡æ ·ï¼ˆå¦‚æœæ•°æ®é‡å¤§ï¼‰
            if len(X) > 10000:
                sample_size = 5000
                indices = np.random.choice(len(X), sample_size, replace=False)
                X_sample = X[indices]

                # æ ‡å‡†åŒ–é‡‡æ ·æ•°æ®
                scaler = StandardScaler()
                X_sample_scaled = scaler.fit_transform(X_sample)

                # MiniBatch K-meansï¼ˆæ›´å¿«ï¼‰
                kmeans = MiniBatchKMeans(
                    n_clusters=n_clusters,
                    random_state=42,
                    batch_size=1000,
                    n_init=3,
                    max_iter=100
                )
                kmeans.fit(X_sample_scaled)

                # é¢„æµ‹æ‰€æœ‰æ•°æ®
                X_scaled = scaler.transform(X)
                clusters = kmeans.predict(X_scaled)
            else:
                # å°æ•°æ®é›†ç›´æ¥å¤„ç†
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)

                kmeans = MiniBatchKMeans(
                    n_clusters=n_clusters,
                    random_state=42,
                    batch_size=min(1000, len(X)),
                    n_init=3
                )
                clusters = kmeans.fit_predict(X_scaled)

            return clusters

        except Exception as e:
            print(f"[WARNING] å¿«é€Ÿèšç±»å¤±è´¥: {e}")
            return np.zeros(len(descriptors_df))

    def _perform_clustering(self, descriptors_df: pd.DataFrame, n_clusters: int = 5) -> np.ndarray:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self._perform_clustering_fast(descriptors_df, n_clusters)

    def _detect_outliers_fast(self, descriptors_df: pd.DataFrame, contamination: float = 0.1) -> np.ndarray:
        """å¿«é€Ÿå¼‚å¸¸å€¼æ£€æµ‹ï¼ˆé‡‡æ ·ç­–ç•¥ï¼‰"""
        try:
            # é€‰æ‹©æ•°å€¼åˆ—
            numeric_cols = descriptors_df.select_dtypes(include=[np.number]).columns
            X = descriptors_df[numeric_cols].fillna(0).values

            # æ•°æ®é‡‡æ ·ç­–ç•¥
            if len(X) > 5000:
                # å¤§æ•°æ®é›†ï¼šè®­ç»ƒé‡‡æ ·ï¼Œé¢„æµ‹å…¨éƒ¨
                sample_size = 2000
                train_indices = np.random.choice(len(X), sample_size, replace=False)
                X_train = X[train_indices]

                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)

                # è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨
                iso_forest = IsolationForest(
                    contamination=contamination,
                    random_state=42,
                    n_jobs=-1,
                    n_estimators=50  # å‡å°‘ä¼°è®¡å™¨æ•°é‡
                )
                iso_forest.fit(X_train_scaled)

                # é¢„æµ‹æ‰€æœ‰æ•°æ®
                X_scaled = scaler.transform(X)
                outliers = iso_forest.predict(X_scaled)
            else:
                # å°æ•°æ®é›†ç›´æ¥å¤„ç†
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)

                iso_forest = IsolationForest(
                    contamination=contamination,
                    random_state=42,
                    n_jobs=-1,
                    n_estimators=50
                )
                outliers = iso_forest.fit_predict(X_scaled)

            return outliers

        except Exception as e:
            print(f"[WARNING] å¿«é€Ÿå¼‚å¸¸å€¼æ£€æµ‹å¤±è´¥: {e}")
            return np.ones(len(descriptors_df))

    def _detect_outliers(self, descriptors_df: pd.DataFrame, contamination: float = 0.1) -> np.ndarray:
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self._detect_outliers_fast(descriptors_df, contamination)

    # def generate_comprehensive_report(self, results: pd.DataFrame, ref_df: pd.DataFrame,
    #                                   lib_df: pd.DataFrame, processing_time: float):
    #     """ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼ˆå¢å¼ºç‰ˆï¼‰"""
    #     print("[INFO] ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")
    #
    #     # ç»Ÿè®¡ä¿¡æ¯
    #     total_compounds = len(results)
    #     threshold = self.config.get("ai_model", {}).get("similarity_threshold", 0.75)
    #     hits = results[results['combined_score'] >= threshold]
    #
    #     # æ€§èƒ½ç»Ÿè®¡
    #     cache_hit_rate = (self.perf_stats['cache_hits'] /
    #                       (self.perf_stats['cache_hits'] + self.perf_stats['cache_misses']) * 100) if \
    #         (self.perf_stats['cache_hits'] + self.perf_stats['cache_misses']) > 0 else 0
    #
    #     # æŠ¥å‘Šå†…å®¹
    #     report_lines = [
    #         "=" * 90,
    #         " è¶…é«˜é€ŸGPU + å¤šçº¿ç¨‹CPUä¼˜åŒ–ç‰ˆ Mitophagyè¯±å¯¼å‰‚è™šæ‹Ÿç­›é€‰æŠ¥å‘Š",
    #         "=" * 90,
    #         "",
    #         "=== ç³»ç»Ÿé…ç½® ===",
    #         f"CPUçº¿ç¨‹æ•°: {self.cpu_pool.max_workers}",
    #         f"å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / (1024 ** 3):.1f} GB",
    #         f"GPUåŠ é€Ÿ: {'å¯ç”¨' if (GPU_AVAILABLE or TORCH_AVAILABLE) else 'ç¦ç”¨'}",
    #     ]
    #
    #     if GPU_AVAILABLE:
    #         report_lines.extend([
    #             f"CuPy GPU: {self.gpu_info.get('device_name', 'Unknown')}",
    #             f"GPUæ˜¾å­˜: {self.gpu_info.get('total_memory', 0):.1f} GB",
    #             f"GPUå†…å­˜é™åˆ¶: {self.gpu_info.get('memory_limit', 0):.1f} GB",
    #         ])
    #
    #     if TORCH_AVAILABLE:
    #         report_lines.append(f"PyTorch GPU: {torch.cuda.get_device_name(0)}")
    #
    #     report_lines.extend([
    #         f"æ€»å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’",
    #         f"å¤„ç†é€Ÿåº¦: {total_compounds / processing_time:.1f} åŒ–åˆç‰©/ç§’",
    #         "",
    #         "=== æ€§èƒ½ç»Ÿè®¡ ===",
    #         f"é¢„å¤„ç†æ—¶é—´: {self.perf_stats['preprocessing_time']:.2f}s",
    #         f"ç‰¹å¾æå–æ—¶é—´: {self.perf_stats['feature_extraction_time']:.2f}s",
    #         f"ç›¸ä¼¼æ€§è®¡ç®—æ—¶é—´: {self.perf_stats['similarity_computation_time']:.2f}s",
    #         f"è¿‡æ»¤æ—¶é—´: {self.perf_stats['filtering_time']:.2f}s",
    #         f"ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}%",
    #         "",
    #         "=== æ•°æ®æ¦‚è§ˆ ===",
    #         f"å‚è€ƒåˆ†å­æ•°: {len(ref_df)}",
    #         f"åº“åˆ†å­æ•°: {len(lib_df)}",
    #         f"æœ‰æ•ˆåŒ–åˆç‰©: {total_compounds}",
    #         "",
    #         "===  ç›¸ä¼¼æ€§æƒé‡ ===",
    #         f"1Dæƒé‡ (Mol2vec/Morgan): {self.config['similarity']['w_1d']:.3f}",
    #         f"2Dæƒé‡ (è¯æ•ˆå›¢): {self.config['similarity']['w_2d']:.3f}",
    #         f"3Dæƒé‡ (å½¢çŠ¶): {self.config['similarity']['w_3d']:.3f}",
    #         "",
    #         "=== ç­›é€‰ç»“æœ ===",
    #         f"ç›¸ä¼¼æ€§é˜ˆå€¼: {threshold:.3f}",
    #         f"å‘½ä¸­åŒ–åˆç‰©æ•°: {len(hits)}",
    #         f"å‘½ä¸­ç‡: {len(hits) / total_compounds * 100:.2f}%",
    #         "",
    #         "=== å¾—åˆ†ç»Ÿè®¡ ===",
    #         f"æœ€é«˜å¾—åˆ†: {results['combined_score'].max():.4f}",
    #         f"å¹³å‡å¾—åˆ†: {results['combined_score'].mean():.4f}",
    #         f"ä¸­ä½æ•°å¾—åˆ†: {results['combined_score'].median():.4f}",
    #         f"æ ‡å‡†å·®: {results['combined_score'].std():.4f}",
    #         f"75åˆ†ä½æ•°: {results['combined_score'].quantile(0.75):.4f}",
    #         f"90åˆ†ä½æ•°: {results['combined_score'].quantile(0.90):.4f}",
    #         "",
    #     ])
    #
    #     # Topå€™é€‰åŒ–åˆç‰©
    #     top_k = min(30, len(results))  # å¢åŠ æ˜¾ç¤ºæ•°é‡
    #     report_lines.extend([
    #         f"=== Top-{top_k} å€™é€‰åŒ–åˆç‰© ===",
    #     ])
    #
    #     for i, (_, row) in enumerate(results.head(top_k).iterrows(), 1):
    #         pains_status = "PAINS+" if row.get('is_pains', False) else "PAINS-"
    #         cns_status = "CNS+" if row.get('cns_compliant', True) else "CNS-"
    #         cluster_info = f"C{row.get('cluster', 'N/A')}" if 'cluster' in row else ""
    #
    #         score_bar = "â–ˆ" * int(row['combined_score'] * 20) + "â–‘" * (20 - int(row['combined_score'] * 20))
    #
    #         report_lines.append(
    #             f"{i:2d}. {row['id'][:15]:15s} | {row.get('name', 'Unknown')[:25]:25s} | "
    #             f"å¾—åˆ†={row['combined_score']:.4f} [{score_bar}] | {pains_status} | {cns_status} | {cluster_info}"
    #         )
    #
    #     # æ€§èƒ½å¯¹æ¯”
    #     estimated_speedup = self.cpu_pool.max_workers
    #     if GPU_AVAILABLE or TORCH_AVAILABLE:
    #         estimated_speedup *= 3  # GPUå¤§çº¦3å€åŠ é€Ÿ
    #
    #     report_lines.extend([
    #         "",
    #         "===  æ€§èƒ½å¯¹æ¯” ===",
    #         f"ç†è®ºåŠ é€Ÿæ¯”: ~{estimated_speedup:.1f}x",
    #         f"å®é™…å¤„ç†é€Ÿåº¦: {total_compounds / processing_time:.1f} åŒ–åˆç‰©/ç§’",
    #         f"CPUçº¿ç¨‹åˆ©ç”¨ç‡: {self.cpu_pool.max_workers}x",
    #         f"GPUåç«¯: {'PyTorch' if TORCH_AVAILABLE else 'CuPy' if GPU_AVAILABLE else 'None'}",
    #         f"å†…å­˜ä¼˜åŒ–: {' å¯ç”¨' if cache_hit_rate > 10 else ' ç¦ç”¨'}",
    #     ])
    #
    #     # è¿‡æ»¤ç»Ÿè®¡
    #     if 'is_pains' in results.columns:
    #         pains_count = results['is_pains'].sum()
    #         pains_rate = pains_count / total_compounds * 100
    #         report_lines.extend([
    #             "",
    #             "=== è¿‡æ»¤ç»Ÿè®¡ ===",
    #             f"PAINSé˜³æ€§: {pains_count} ({pains_rate:.1f}%)",
    #         ])
    #
    #     if 'cns_compliant' in results.columns:
    #         cns_pass = results['cns_compliant'].sum()
    #         cns_rate = cns_pass / total_compounds * 100
    #         report_lines.append(f"CNSåˆè§„: {cns_pass} ({cns_rate:.1f}%)")
    #
    #     # èšç±»åˆ†æ
    #     if 'cluster' in results.columns:
    #         report_lines.extend([
    #             "",
    #             "=== ğŸ”¬ èšç±»åˆ†æ ===",
    #         ])
    #         cluster_counts = results['cluster'].value_counts().sort_index()
    #         for cluster_id, count in cluster_counts.items():
    #             percentage = count / total_compounds * 100
    #             bar = "â–ˆ" * int(percentage / 5) + "â–‘" * (20 - int(percentage / 5))
    #             report_lines.append(f"èšç±» {cluster_id}: {count:4d} ä¸ªåŒ–åˆç‰© ({percentage:5.1f}%) [{bar}]")
    #
    #     # å†…å­˜å’Œèµ„æºä½¿ç”¨
    #     memory_info = psutil.virtual_memory()
    #     report_lines.extend([
    #         "",
    #         "===èµ„æºä½¿ç”¨ ===",
    #         f"ç³»ç»Ÿå†…å­˜: {memory_info.total / (1024 ** 3):.1f} GB",
    #         f"å¯ç”¨å†…å­˜: {memory_info.available / (1024 ** 3):.1f} GB",
    #         f"å†…å­˜ä½¿ç”¨ç‡: {memory_info.percent:.1f}%",
    #         f"CPUæ ¸å¿ƒæ•°: {os.cpu_count()}",
    #     ])
    #
    #     # ä¿å­˜æŠ¥å‘Š
    #     report_path = os.path.join(self.output_dir, self.config["output"]["report_txt"])
    #     with open(report_path, 'w', encoding='utf-8') as f:
    #         f.write('\n'.join(report_lines))
    #
    #     print(f"[INFO] æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    #
    #     # ä¿å­˜è¯¦ç»†å¤„ç†ä¿¡æ¯
    #     processing_info = {
    #         'processing_time_seconds': processing_time,
    #         'performance_stats': self.perf_stats,
    #         'cpu_threads': self.cpu_pool.max_workers,
    #         'gpu_enabled': GPU_AVAILABLE or TORCH_AVAILABLE,
    #         'gpu_backend': 'PyTorch' if TORCH_AVAILABLE else 'CuPy' if GPU_AVAILABLE else 'None',
    #         'gpu_info': getattr(self, 'gpu_info', {}),
    #         'system_info': {
    #             'cpu_count': os.cpu_count(),
    #             'memory_total_gb': psutil.virtual_memory().total / (1024 ** 3),
    #             'memory_available_gb': psutil.virtual_memory().available / (1024 ** 3),
    #         },
    #         'data_stats': {
    #             'total_compounds': total_compounds,
    #             'reference_compounds': len(ref_df),
    #             'library_compounds': len(lib_df),
    #             'hits_count': len(hits),
    #             'hit_rate': len(hits) / total_compounds if total_compounds > 0 else 0,
    #         },
    #         'score_stats': {
    #             'max_score': float(results['combined_score'].max()),
    #             'mean_score': float(results['combined_score'].mean()),
    #             'median_score': float(results['combined_score'].median()),
    #             'std_score': float(results['combined_score'].std()),
    #         },
    #         'config_used': self.config
    #     }
    #
    #     info_path = os.path.join(self.output_dir, "processing_info.json")
    #     import json
    #     with open(info_path, 'w') as f:
    #         json.dump(processing_info, f, indent=2)
    #
    #     # ç”Ÿæˆæ€§èƒ½å›¾è¡¨ï¼ˆå¦‚æœmatplotlibå¯ç”¨ï¼‰
    #     try:
    #         import matplotlib.pyplot as plt
    #         self._generate_performance_plots(results, processing_info)
    #     except ImportError:
    #         print("[INFO] matplotlibä¸å¯ç”¨ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")

    def generate_comprehensive_report(self, results: pd.DataFrame, ref_df: pd.DataFrame,
                                      lib_df: pd.DataFrame, processing_time: float):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼ˆä¿®å¤ç‰ˆï¼‰"""
        print("[INFO] ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")

        # ç»Ÿè®¡ä¿¡æ¯
        total_compounds = len(results)
        threshold = self.config.get("ai_model", {}).get("similarity_threshold", 0.75)
        hits = results[results['combined_score'] >= threshold]

        # æ€§èƒ½ç»Ÿè®¡
        cache_hit_rate = (self.perf_stats.get('cache_hits', 0) /
                          (self.perf_stats.get('cache_hits', 0) + self.perf_stats.get('cache_misses', 1)) * 100)

        # æŠ¥å‘Šå†…å®¹
        report_lines = [
            "=" * 90,
            " è¶…é«˜é€ŸGPU + å¤šçº¿ç¨‹CPUä¼˜åŒ–ç‰ˆ Mitophagyè¯±å¯¼å‰‚è™šæ‹Ÿç­›é€‰æŠ¥å‘Š",
            "=" * 90,
            "",
            "=== ç³»ç»Ÿé…ç½® ===",
            f"CPUçº¿ç¨‹æ•°: {self.cpu_pool.max_workers}",
            f"å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / (1024 ** 3):.1f} GB",
            f"GPUåŠ é€Ÿ: {'å¯ç”¨' if (GPU_AVAILABLE or TORCH_AVAILABLE) else 'ç¦ç”¨'}",
            f"æ€»å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’",
            f"å¤„ç†é€Ÿåº¦: {total_compounds / processing_time:.1f} åŒ–åˆç‰©/ç§’",
            "",
            "===  æ•°æ®æ¦‚è§ˆ ===",
            f"å‚è€ƒåˆ†å­æ•°: {len(ref_df)}",
            f"åº“åˆ†å­æ•°: {len(lib_df)}",
            f"æœ‰æ•ˆåŒ–åˆç‰©: {total_compounds}",
            "",
            "=== ç­›é€‰ç»“æœ ===",
            f"ç›¸ä¼¼æ€§é˜ˆå€¼: {threshold:.3f}",
            f"å‘½ä¸­åŒ–åˆç‰©æ•°: {len(hits)}",
            f"å‘½ä¸­ç‡: {len(hits) / total_compounds * 100:.2f}%" if total_compounds > 0 else "å‘½ä¸­ç‡: 0%",
            "",
            "=== å¾—åˆ†ç»Ÿè®¡ ===",
            f"æœ€é«˜å¾—åˆ†: {results['combined_score'].max():.4f}" if len(results) > 0 else "æœ€é«˜å¾—åˆ†: N/A",
            f"å¹³å‡å¾—åˆ†: {results['combined_score'].mean():.4f}" if len(results) > 0 else "å¹³å‡å¾—åˆ†: N/A",
            f"ä¸­ä½æ•°å¾—åˆ†: {results['combined_score'].median():.4f}" if len(results) > 0 else "ä¸­ä½æ•°å¾—åˆ†: N/A",
            f"æ ‡å‡†å·®: {results['combined_score'].std():.4f}" if len(results) > 0 else "æ ‡å‡†å·®: N/A",
            "",
        ]

        # Topå€™é€‰åŒ–åˆç‰© - ä¿®å¤ç‰ˆæœ¬
        if len(results) > 0:
            top_k = min(20, len(results))
            report_lines.extend([
                f"=== Top-{top_k} å€™é€‰åŒ–åˆç‰© ===",
            ])

            for i, (_, row) in enumerate(results.head(top_k).iterrows(), 1):
                try:
                    # å®‰å…¨çš„å­—ç¬¦ä¸²å¤„ç†
                    row_id = str(row.get('id', 'Unknown'))[:15]
                    row_name = str(row.get('name', 'Unknown'))[:25]
                    combined_score = float(row.get('combined_score', 0.0))

                    pains_status = "PAINS+" if row.get('is_pains', False) else "PAINS-"
                    cns_status = "CNS+" if row.get('cns_compliant', True) else "CNS-"
                    cluster_info = f"C{row.get('cluster', 'N/A')}" if 'cluster' in row else ""

                    score_bar = "â–ˆ" * int(combined_score * 20) + "â–‘" * (20 - int(combined_score * 20))

                    report_lines.append(
                        f"{i:2d}. {row_id:15s} | {row_name:25s} | "
                        f"å¾—åˆ†={combined_score:.4f} [{score_bar}] | {pains_status} | {cns_status} | {cluster_info}"
                    )
                except Exception as e:
                    print(f"[WARNING] å¤„ç†ç¬¬{i}ä¸ªåŒ–åˆç‰©ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                    report_lines.append(f"{i:2d}. æ•°æ®å¤„ç†é”™è¯¯")

        # æ€§èƒ½å¯¹æ¯”
        estimated_speedup = self.cpu_pool.max_workers
        if GPU_AVAILABLE or TORCH_AVAILABLE:
            estimated_speedup *= 3

        report_lines.extend([
            "",
            "===  æ€§èƒ½ç»Ÿè®¡ ===",
            f"ç†è®ºåŠ é€Ÿæ¯”: ~{estimated_speedup:.1f}x",
            f"å®é™…å¤„ç†é€Ÿåº¦: {total_compounds / processing_time:.1f} åŒ–åˆç‰©/ç§’" if processing_time > 0 else "å¤„ç†é€Ÿåº¦: N/A",
            f"CPUçº¿ç¨‹åˆ©ç”¨ç‡: {self.cpu_pool.max_workers}x",
            f"GPUåç«¯: {'PyTorch' if TORCH_AVAILABLE else 'CuPy' if GPU_AVAILABLE else 'None'}",
            f"ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}%",
        ])

        # è¿‡æ»¤ç»Ÿè®¡
        if len(results) > 0:
            if 'is_pains' in results.columns:
                pains_count = results['is_pains'].sum()
                pains_rate = pains_count / total_compounds * 100
                report_lines.extend([
                    "",
                    "===è¿‡æ»¤ç»Ÿè®¡ ===",
                    f"PAINSé˜³æ€§: {pains_count} ({pains_rate:.1f}%)",
                ])

            if 'cns_compliant' in results.columns:
                cns_pass = results['cns_compliant'].sum()
                cns_rate = cns_pass / total_compounds * 100
                report_lines.append(f"CNSåˆè§„: {cns_pass} ({cns_rate:.1f}%)")

        # ç³»ç»Ÿèµ„æºä¿¡æ¯
        try:
            memory_info = psutil.virtual_memory()
            report_lines.extend([
                "",
                "=== èµ„æºä½¿ç”¨ ===",
                f"ç³»ç»Ÿå†…å­˜: {memory_info.total / (1024 ** 3):.1f} GB",
                f"å¯ç”¨å†…å­˜: {memory_info.available / (1024 ** 3):.1f} GB",
                f"å†…å­˜ä½¿ç”¨ç‡: {memory_info.percent:.1f}%",
                f"CPUæ ¸å¿ƒæ•°: {os.cpu_count()}",
            ])
        except Exception as e:
            print(f"[WARNING] è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {e}")

        # ä¿å­˜æŠ¥å‘Š
        try:
            report_path = os.path.join(self.output_dir, self.config["output"]["report_txt"])
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_lines))
            print(f"[INFO] æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        except Exception as e:
            print(f"[WARNING] æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")

        # ä¿å­˜å¤„ç†ä¿¡æ¯
        try:
            processing_info = {
                'processing_time_seconds': processing_time,
                'performance_stats': self.perf_stats,
                'cpu_threads': self.cpu_pool.max_workers,
                'gpu_enabled': GPU_AVAILABLE or TORCH_AVAILABLE,
                'gpu_backend': 'PyTorch' if TORCH_AVAILABLE else 'CuPy' if GPU_AVAILABLE else 'None',
                'system_info': {
                    'cpu_count': os.cpu_count(),
                    'memory_total_gb': psutil.virtual_memory().total / (1024 ** 3),
                    'memory_available_gb': psutil.virtual_memory().available / (1024 ** 3),
                },
                'data_stats': {
                    'total_compounds': total_compounds,
                    'reference_compounds': len(ref_df),
                    'library_compounds': len(lib_df),
                    'hits_count': len(hits),
                    'hit_rate': len(hits) / total_compounds if total_compounds > 0 else 0,
                },
                'config_used': self.config
            }

            info_path = os.path.join(self.output_dir, "processing_info.json")
            import json
            with open(info_path, 'w') as f:
                json.dump(processing_info, f, indent=2)
            print(f"[INFO] å¤„ç†ä¿¡æ¯å·²ä¿å­˜: {info_path}")
        except Exception as e:
            print(f"[WARNING] å¤„ç†ä¿¡æ¯ä¿å­˜å¤±è´¥: {e}")




    def _generate_performance_plots(self, results: pd.DataFrame, processing_info: Dict):
        """ç”Ÿæˆæ€§èƒ½åˆ†æå›¾è¡¨"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns

            plt.style.use('default')
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('è™šæ‹Ÿç­›é€‰æ€§èƒ½åˆ†æ', fontsize=16, fontweight='bold')

            # 1. å¾—åˆ†åˆ†å¸ƒç›´æ–¹å›¾
            axes[0, 0].hist(results['combined_score'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
            axes[0, 0].set_xlabel('ç»„åˆç›¸ä¼¼æ€§å¾—åˆ†')
            axes[0, 0].set_ylabel('åŒ–åˆç‰©æ•°é‡')
            axes[0, 0].set_title('å¾—åˆ†åˆ†å¸ƒ')
            axes[0, 0].grid(True, alpha=0.3)

            # 2. å¤„ç†æ—¶é—´é¥¼å›¾
            perf_stats = processing_info['performance_stats']
            labels = ['é¢„å¤„ç†', 'ç‰¹å¾æå–', 'ç›¸ä¼¼æ€§è®¡ç®—', 'è¿‡æ»¤', 'å…¶ä»–']
            sizes = [
                perf_stats['preprocessing_time'],
                perf_stats['feature_extraction_time'],
                perf_stats['similarity_computation_time'],
                perf_stats['filtering_time'],
                max(0, processing_info['processing_time_seconds'] - sum([
                    perf_stats['preprocessing_time'],
                    perf_stats['feature_extraction_time'],
                    perf_stats['similarity_computation_time'],
                    perf_stats['filtering_time']
                ]))
            ]

            axes[0, 1].pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
            axes[0, 1].set_title('å¤„ç†æ—¶é—´åˆ†å¸ƒ')

            # 3. TopåŒ–åˆç‰©å¾—åˆ†æ¡å½¢å›¾
            top_20 = results.head(20)
            axes[1, 0].barh(range(len(top_20)), top_20['combined_score'], color='lightcoral')
            axes[1, 0].set_yticks(range(len(top_20)))
            axes[1, 0].set_yticklabels([f"{row['id'][:10]}" for _, row in top_20.iterrows()])
            axes[1, 0].set_xlabel('ç»„åˆå¾—åˆ†')
            axes[1, 0].set_title('Top-20 å€™é€‰åŒ–åˆç‰©')
            axes[1, 0].grid(True, alpha=0.3)

            # 4. ç³»ç»Ÿèµ„æºä½¿ç”¨
            resource_labels = ['CPUçº¿ç¨‹', 'GPU', 'å†…å­˜(GB)', 'å¤„ç†é€Ÿåº¦(åŒ–åˆç‰©/ç§’)']
            resource_values = [
                processing_info['cpu_threads'],
                1 if processing_info['gpu_enabled'] else 0,
                processing_info['system_info']['memory_available_gb'],
                len(results) / processing_info['processing_time_seconds']
            ]

            bars = axes[1, 1].bar(resource_labels, resource_values, color=['gold', 'red', 'green', 'blue'])
            axes[1, 1].set_title('ç³»ç»Ÿèµ„æºä½¿ç”¨')
            axes[1, 1].set_ylabel('èµ„æºé‡')

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, resource_values):
                height = bar.get_height()
                axes[1, 1].text(bar.get_x() + bar.get_width() / 2., height + height * 0.01,
                                f'{value:.1f}', ha='center', va='bottom')

            plt.tight_layout()

            # ä¿å­˜å›¾è¡¨
            plot_path = os.path.join(self.output_dir, "performance_analysis.png")
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"[INFO] æ€§èƒ½åˆ†æå›¾è¡¨å·²ä¿å­˜: {plot_path}")

        except Exception as e:
            print(f"[WARNING] å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")

    def run_virtual_screening_ultra_fast(self):
        """æ‰§è¡Œè¶…é«˜é€Ÿè™šæ‹Ÿç­›é€‰æµç¨‹"""
        start_time = time.time()

        print("=" * 90)
        print("GPU + å¤šçº¿ç¨‹CPUä¼˜åŒ–ç‰ˆè™šæ‹Ÿç­›é€‰ç³»ç»Ÿå¯åŠ¨")
        print("=" * 90)
        print(f"CPUçº¿ç¨‹æ± : {self.cpu_pool.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
        print(f"å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / (1024 ** 3):.1f} GB")

        if TORCH_AVAILABLE:
            print(f"PyTorch GPU: {torch.cuda.get_device_name(0)}")
        elif GPU_AVAILABLE:
            print(f"CuPy GPU: {self.gpu_info.get('device_name', 'Unknown')}")
        else:
            print("CPUæ¨¡å¼")
        print("=" * 90)

        try:
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æµå¼å¤„ç†
            use_streaming = self.config.get("performance", {}).get("use_streaming", False)

            if use_streaming:
                # æµå¼å¤„ç†ï¼ˆå¤„ç†è¶…å¤§æ•°æ®é›†ï¼‰
                ref_df, lib_stream = self.load_and_preprocess_data_streaming()
                return self._run_streaming_screening(ref_df, lib_stream, start_time)
            else:
                # æ ‡å‡†å¤„ç†
                return self._run_standard_screening(start_time)

        except Exception as e:
            print(f"[ERROR] è¶…é«˜é€Ÿè™šæ‹Ÿç­›é€‰å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
        finally:
            # æ¸…ç†èµ„æº
            self._cleanup_resources()

    def run_virtual_screening(self):
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self.run_virtual_screening_ultra_fast()

    def _run_standard_screening(self, start_time: float):
        """æ ‡å‡†ç­›é€‰æµç¨‹"""
        # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
        ref_df, lib_df = self.load_and_preprocess_data()

        # 2. ç‰¹å¾æå–
        ref_features = self.extract_molecular_features_optimized(ref_df, "å‚è€ƒåˆ†å­")
        lib_features = self.extract_molecular_features_optimized(lib_df, "åº“åˆ†å­")

        # 3. å¤šç»´ç›¸ä¼¼æ€§è®¡ç®—
        similarities = self.compute_multi_dimensional_similarity_ultra_fast(ref_features, lib_features)

        # 4. ç›¸ä¼¼æ€§èšåˆ
        scores = self.aggregate_similarities_ultra_fast(similarities)

        # 5. AIå¢å¼ºè¿‡æ»¤
        results = self.apply_ai_enhanced_filtering_ultra_fast(lib_df, scores, lib_features)

        # 6. ä¿å­˜ç»“æœ
        results_path = os.path.join(self.output_dir, self.config["output"]["hits_csv"])
        results.to_csv(results_path, index=False)
        print(f"[INFO] ç»“æœå·²ä¿å­˜: {results_path}")

        # 7. ç”ŸæˆæŠ¥å‘Š
        processing_time = time.time() - start_time
        self.generate_comprehensive_report(results, ref_df, lib_df, processing_time)

        # 8. è¾“å‡ºæ‘˜è¦
        self._print_final_summary(results, processing_time)

        return results

    def _run_streaming_screening(self, ref_df: pd.DataFrame, lib_stream: Iterator, start_time: float):
        """æµå¼ç­›é€‰æµç¨‹ï¼ˆå¤„ç†è¶…å¤§æ•°æ®é›†ï¼‰"""
        print("[INFO] å¯åŠ¨æµå¼ç­›é€‰æ¨¡å¼...")

        # é¢„æå–å‚è€ƒåˆ†å­ç‰¹å¾
        ref_features = self.extract_molecular_features_optimized(ref_df, "å‚è€ƒåˆ†å­")

        all_results = []
        chunk_count = 0

        for lib_chunk in lib_stream:
            chunk_count += 1
            print(f"[INFO] å¤„ç†ç¬¬ {chunk_count} ä¸ªæ•°æ®å— ({len(lib_chunk)} ä¸ªåŒ–åˆç‰©)...")

            try:
                # ç‰¹å¾æå–
                lib_features = self.extract_molecular_features_optimized(lib_chunk, f"åº“åˆ†å­å—{chunk_count}")

                # ç›¸ä¼¼æ€§è®¡ç®—
                similarities = self.compute_multi_dimensional_similarity_ultra_fast(ref_features, lib_features)

                # èšåˆå’Œè¿‡æ»¤
                scores = self.aggregate_similarities_ultra_fast(similarities)
                chunk_results = self.apply_ai_enhanced_filtering_ultra_fast(lib_chunk, scores, lib_features)

                # åªä¿ç•™é«˜åˆ†åŒ–åˆç‰©ï¼ˆèŠ‚çœå†…å­˜ï¼‰
                threshold = self.config.get("ai_model", {}).get("similarity_threshold", 0.5)
                high_score_results = chunk_results[chunk_results['combined_score'] >= threshold]

                if len(high_score_results) > 0:
                    all_results.append(high_score_results)
                    print(f"[INFO] å— {chunk_count} å‘ç° {len(high_score_results)} ä¸ªå€™é€‰åŒ–åˆç‰©")
                else:
                    print(f"[INFO] å— {chunk_count} æœªå‘ç°ç¬¦åˆæ¡ä»¶çš„åŒ–åˆç‰©")

                # å†…å­˜æ¸…ç†
                del lib_features, similarities, scores, chunk_results
                gc.collect()

            except Exception as e:
                print(f"[WARNING] å— {chunk_count} å¤„ç†å¤±è´¥: {e}")
                continue

        # åˆå¹¶æ‰€æœ‰ç»“æœ
        if all_results:
            print("[INFO] åˆå¹¶æµå¼å¤„ç†ç»“æœ...")
            final_results = pd.concat(all_results, ignore_index=True)

            # æœ€ç»ˆæ’åº
            final_results = final_results.sort_values('combined_score', ascending=False).reset_index(drop=True)

            # é™åˆ¶ç»“æœæ•°é‡ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
            max_results = self.config.get("performance", {}).get("max_results", 50000)
            if len(final_results) > max_results:
                print(f"[INFO] é™åˆ¶ç»“æœæ•°é‡è‡³ {max_results}")
                final_results = final_results.head(max_results)
        else:
            print("[WARNING] æµå¼å¤„ç†æœªå‘ç°ä»»ä½•å€™é€‰åŒ–åˆç‰©")
            final_results = pd.DataFrame()

        # ä¿å­˜å’ŒæŠ¥å‘Š
        if len(final_results) > 0:
            results_path = os.path.join(self.output_dir, self.config["output"]["hits_csv"])
            final_results.to_csv(results_path, index=False)
            print(f"[INFO] æµå¼å¤„ç†ç»“æœå·²ä¿å­˜: {results_path}")

            # ç”ŸæˆæŠ¥å‘Šï¼ˆä½¿ç”¨ä¼°ç®—çš„åº“å¤§å°ï¼‰
            processing_time = time.time() - start_time
            estimated_lib_size = chunk_count * 10000  # ä¼°ç®—
            mock_lib_df = pd.DataFrame({'estimated_size': [estimated_lib_size]})
            self.generate_comprehensive_report(final_results, ref_df, mock_lib_df, processing_time)

            self._print_final_summary(final_results, processing_time)

        return final_results

    def _cleanup_resources(self):
        """æ¸…ç†ç³»ç»Ÿèµ„æº"""
        try:
            # æ¸…ç†GPUå†…å­˜
            if GPU_AVAILABLE:
                mempool = cp.get_default_memory_pool()
                mempool.free_all_blocks()
                print("[INFO] CuPy GPUå†…å­˜å·²æ¸…ç†")

            if TORCH_AVAILABLE:
                torch.cuda.empty_cache()
                print("[INFO] PyTorch GPUç¼“å­˜å·²æ¸…ç†")

            # æ¸…ç†Pythonå†…å­˜
            gc.collect()

            # æ¸…ç†ç¼“å­˜
            if hasattr(self, 'feature_cache'):
                self.feature_cache.cache.clear()

        except Exception as e:
            print(f"[WARNING] èµ„æºæ¸…ç†æ—¶å‡ºé”™: {e}")

    def _print_final_summary(self, results: pd.DataFrame, processing_time: float):
        """æ‰“å°æœ€ç»ˆæ‘˜è¦"""
        threshold = self.config.get("ai_model", {}).get("similarity_threshold", 0.75)
        hits = results[results['combined_score'] >= threshold] if len(results) > 0 else pd.DataFrame()

        print("\n" + "=" * 90)
        print(" è¶…é«˜é€Ÿè™šæ‹Ÿç­›é€‰å®Œæˆ")
        print("=" * 90)
        print(f"ï¸  å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        print(f" CPUçº¿ç¨‹: {self.cpu_pool.max_workers}")
        print(f" GPUåŠ é€Ÿ: {' å¯ç”¨' if (GPU_AVAILABLE or TORCH_AVAILABLE) else ' ç¦ç”¨'}")
        print(f"æ€»åŒ–åˆç‰©: {len(results):,}")
        print(f" å‘½ä¸­æ•° (â‰¥{threshold}): {len(hits):,}")
        print(f" æœ€é«˜å¾—åˆ†: {results['combined_score'].max():.4f}" if len(results) > 0 else " æœ€é«˜å¾—åˆ†: N/A")
        print(f" è¾“å‡ºç›®å½•: {self.output_dir}")

        # æ€§èƒ½æŒ‡æ ‡
        if processing_time > 0:
            compounds_per_sec = len(results) / processing_time
            print(f" å¤„ç†é€Ÿåº¦: {compounds_per_sec:,.1f} åŒ–åˆç‰©/ç§’")

        # ç¼“å­˜æ€§èƒ½
        if hasattr(self, 'perf_stats'):
            cache_hits = self.perf_stats.get('cache_hits', 0)
            cache_misses = self.perf_stats.get('cache_misses', 0)
            if cache_hits + cache_misses > 0:
                cache_hit_rate = cache_hits / (cache_hits + cache_misses) * 100
                print(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}%")

        print("=" * 90)


def _safe_tensor_check(self, data):
    """å®‰å…¨çš„å¼ é‡/æ•°ç»„æ£€æŸ¥å‡½æ•°"""
    if data is None:
        return True, "none"

    # PyTorch å¼ é‡æ£€æŸ¥
    if TORCH_AVAILABLE and torch.is_tensor(data):
        return data.numel() == 0, "torch_tensor"

    # CuPy æ•°ç»„æ£€æŸ¥
    if GPU_AVAILABLE:
        try:
            import cupy as cp
            if isinstance(data, cp.ndarray):
                return data.size == 0, "cupy_array"
        except:
            pass

    # NumPy æ•°ç»„æ£€æŸ¥
    if isinstance(data, np.ndarray):
        return data.size == 0, "numpy_array"

    # æ™®é€šåˆ—è¡¨æ£€æŸ¥
    if isinstance(data, list):
        return len(data) == 0, "list"

    # å…¶ä»–ç±»å‹
    try:
        return not bool(data), "other"
    except:
        return False, "unknown"


# åœ¨éœ€è¦çš„åœ°æ–¹ä½¿ç”¨
def _process_descriptors_dataframe(self, descriptors_list) -> pd.DataFrame:
    """å¿«é€Ÿå¤„ç†æè¿°ç¬¦åˆ—è¡¨ä¸ºDataFrame - ä½¿ç”¨å®‰å…¨æ£€æŸ¥"""

    is_empty, data_type = self._safe_tensor_check(descriptors_list)

    if is_empty:
        print(f"[INFO] æè¿°ç¬¦æ•°æ®ä¸ºç©º (ç±»å‹: {data_type})")
        return pd.DataFrame()

    # æ ¹æ®æ•°æ®ç±»å‹è¿›è¡Œç›¸åº”å¤„ç†
    # [å…¶ä½™å¤„ç†é€»è¾‘...]
def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def optimize_system_settings():
    """ä¼˜åŒ–ç³»ç»Ÿè®¾ç½®"""
    try:
        # è®¾ç½®CPUäº²å’Œæ€§ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(os, 'sched_setaffinity'):
            available_cpus = list(range(os.cpu_count()))
            os.sched_setaffinity(0, available_cpus)

        # è®¾ç½®è¿›ç¨‹ä¼˜å…ˆçº§ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(os, 'nice'):
            os.nice(-5)  # æé«˜ä¼˜å…ˆçº§

        # è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–
        os.environ['OMP_NUM_THREADS'] = str(min(8, os.cpu_count()))
        os.environ['MKL_NUM_THREADS'] = str(min(8, os.cpu_count()))
        os.environ['NUMEXPR_NUM_THREADS'] = str(min(8, os.cpu_count()))

        print("[INFO] ç³»ç»Ÿä¼˜åŒ–è®¾ç½®å·²åº”ç”¨")

    except Exception as e:
        print(f"[WARNING] ç³»ç»Ÿä¼˜åŒ–è®¾ç½®å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description=" è¶…é«˜é€ŸGPU + å¤šçº¿ç¨‹CPUä¼˜åŒ–ç‰ˆè™šæ‹Ÿç­›é€‰")
    parser.add_argument("--config", type=str, required=True, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--threads", type=int, help="CPUçº¿ç¨‹æ•°ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰")
    parser.add_argument("--gpu", action="store_true", help="å¼ºåˆ¶å¯ç”¨GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰")
    parser.add_argument("--streaming", action="store_true", help="å¯ç”¨æµå¼å¤„ç†ï¼ˆå¤§æ•°æ®é›†ï¼‰")
    parser.add_argument("--optimize", action="store_true", help="åº”ç”¨ç³»ç»Ÿä¼˜åŒ–è®¾ç½®")

    args = parser.parse_args()

    # åº”ç”¨ç³»ç»Ÿä¼˜åŒ–
    if args.optimize:
        optimize_system_settings()

    # åŠ è½½é…ç½®
    config = load_config(args.config)

    # è®¾ç½®æµå¼å¤„ç†
    if args.streaming:
        config.setdefault("performance", {})["use_streaming"] = True

    # è®¾ç½®çº¿ç¨‹æ•°
    if args.threads:
        original_cpu_count = os.cpu_count()
        os.environ['OMP_NUM_THREADS'] = str(args.threads)
        print(f"[INFO] è®¾ç½®CPUçº¿ç¨‹æ•°: {args.threads} (ç³»ç»Ÿ: {original_cpu_count})")

    # åˆ›å»ºè¶…é«˜é€Ÿè™šæ‹Ÿç­›é€‰ç³»ç»Ÿ
    vs_system = UltraFastVirtualScreening(config)

    # è¿è¡Œè™šæ‹Ÿç­›é€‰
    results = vs_system.run_virtual_screening_ultra_fast()

    return results


if __name__ == "__main__":
    main()